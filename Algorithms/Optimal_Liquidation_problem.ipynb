{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimal_Liquidation_problem.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_Tu-Uq8Tz2p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "77730c77-dd3c-4a57-b20d-7a32f9d034eb"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "\n",
        "from statsmodels.iolib.table import SimpleTable\n",
        "from statsmodels.compat.python import zip_longest\n",
        "from statsmodels.iolib.tableformatting import fmt_2cols\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_yheh_4t27x"
      },
      "source": [
        "ANNUAL_VOLAT = 0.12                                # Annual volatility in stock price\n",
        "BID_ASK_SP = 1 / 8                                 # Bid-ask spread\n",
        "DAILY_TRADE_VOL = 5e6                              # Average Daily trading volume  \n",
        "TRAD_DAYS = 250                                    # Number of trading days in a year\n",
        "DAILY_VOLAT = ANNUAL_VOLAT / np.sqrt(TRAD_DAYS)    # Daily volatility in stock price\n",
        "\n",
        "\n",
        "# ----------------------------- Parameters for the Almgren and Chriss Optimal Execution Model ----------------------------- #\n",
        "\n",
        "TOTAL_SHARES = 1000000                                               # Total number of shares to sell\n",
        "STARTING_PRICE = 50                                                  # Starting price per share\n",
        "LLAMBDA = 1e-6                                                       # Trader's risk aversion\n",
        "LIQUIDATION_TIME = 60                                                # How many days to sell all the shares. \n",
        "NUM_N = 60                                                           # Number of trades\n",
        "EPSILON = BID_ASK_SP / 2                                             # Fixed Cost of Selling.\n",
        "SINGLE_STEP_VARIANCE = (DAILY_VOLAT  * STARTING_PRICE) ** 2          # Calculate single step variance\n",
        "ETA = BID_ASK_SP / (0.01 * DAILY_TRADE_VOL)                          # Price Impact for Each 1% of Daily Volume Traded\n",
        "GAMMA = BID_ASK_SP / (0.1 * DAILY_TRADE_VOL)                         # Permanent Impact Constant\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------- #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyrjJbLGt78Z"
      },
      "source": [
        "class MarketEnvironment():\n",
        "\n",
        "    def __init__(self, seed = 0, lqd_time = LIQUIDATION_TIME, num_tr = NUM_N, lambd = LLAMBDA):\n",
        "        random.seed(seed)\n",
        "        self.anv = ANNUAL_VOLAT\n",
        "        self.basp = BID_ASK_SP\n",
        "        self.dtv = DAILY_TRADE_VOL\n",
        "        self.dpv = DAILY_VOLAT\n",
        "        \n",
        "        # Initialize the Almgren-Chriss parameters so we can access them later\n",
        "        self.total_shares = TOTAL_SHARES\n",
        "        self.startingPrice = STARTING_PRICE\n",
        "        self.llambda = lambd\n",
        "        self.liquidation_time = lqd_time\n",
        "        self.num_n = num_tr\n",
        "        self.epsilon = EPSILON\n",
        "        self.singleStepVariance = SINGLE_STEP_VARIANCE\n",
        "        self.eta = ETA\n",
        "        self.gamma = GAMMA\n",
        "        \n",
        "        # Calculate some Almgren-Chriss parameters\n",
        "        self.tau = self.liquidation_time / self.num_n \n",
        "        self.eta_hat = self.eta - (0.5 * self.gamma * self.tau)\n",
        "        self.kappa_hat = np.sqrt((self.llambda * self.singleStepVariance) / self.eta_hat)\n",
        "        self.kappa = np.arccosh((((self.kappa_hat ** 2) * (self.tau ** 2)) / 2) + 1) / self.tau\n",
        "\n",
        "        # Set the variables for the initial state\n",
        "        self.shares_remaining = self.total_shares\n",
        "        self.timeHorizon = self.num_n\n",
        "        self.logReturns = collections.deque(np.zeros(6))\n",
        "        \n",
        "        # Set the initial impacted price to the starting price\n",
        "        self.prevImpactedPrice = self.startingPrice\n",
        "\n",
        "        # Set the initial transaction state to False\n",
        "        self.transacting = False\n",
        "        \n",
        "        # Set a variable to keep trak of the trade number\n",
        "        self.k = 0\n",
        "\n",
        "    def reset(self, seed = 0, liquid_time = LIQUIDATION_TIME, num_trades = NUM_N, lamb = LLAMBDA):\n",
        "        # Initialize the environment with the given parameters\n",
        "        self.__init__(seed = seed, lqd_time = liquid_time, num_tr = num_trades, lambd = lamb)\n",
        "        \n",
        "        # Set the initial state to [0,0,0,0,0,0,1,1]\n",
        "        self.initial_state = np.array(list(self.logReturns) + [self.timeHorizon / self.num_n, self.shares_remaining / self.total_shares])\n",
        "\n",
        "        return self.initial_state\n",
        "\n",
        "    def start_transactions(self):\n",
        "        self.transacting = True\n",
        "        self.tolerance = 1\n",
        "        self.prev_price = self.startingPrice\n",
        "        self.totalSSSQ = 0\n",
        "        \n",
        "        # Set the initial square of the remaing shares to sell to zero\n",
        "        self.totalSRSQ = 0\n",
        "        \n",
        "        # Set the initial AC utility\n",
        "        self.prevUtility = self.compute_AC_utility(self.total_shares)\n",
        "\n",
        "    def permanentImpact(self, sharesToSell):\n",
        "        # Calculate the permanent impact according to equations (6) and (1) of the AC paper\n",
        "        pi = self.gamma * sharesToSell\n",
        "        return pi\n",
        "\n",
        "    \n",
        "    def temporaryImpact(self, sharesToSell):\n",
        "        # Calculate the temporary impact according to equation (7) of the AC paper\n",
        "        ti = (self.epsilon * np.sign(sharesToSell)) + ((self.eta / self.tau) * sharesToSell)\n",
        "        return ti\n",
        "\n",
        "    def compute_AC_utility(self, sharesToSell):\n",
        "        if self.liquidation_time == 0:\n",
        "            return 0\n",
        "        else :\n",
        "            E = self.get_AC_expected_shortfall(sharesToSell)\n",
        "            V = self.get_AC_variance(sharesToSell)\n",
        "            return E + self.llambda * V \n",
        "\n",
        "    def get_AC_expected_shortfall(self, sharesToSell):\n",
        "        first = 0.5 * self.gamma * (sharesToSell ** 2) + self.epsilon * sharesToSell\n",
        "        numerator_first = self.eta_hat * (sharesToSell ** 2) * (np.tanh(0.5 * self.kappa * self.tau))\n",
        "        numerator_sum = self.tau * np.sinh(2 * self.kappa * self.liquidation_time) + 2 * self.liquidation_time * np.sinh(self.kappa * self.tau)\n",
        "        denominator = 2 * self.tau ** 2 * (np.sinh(self.kappa * self.liquidation_time) ** 2)\n",
        "\n",
        "        return first + (numerator_first * numerator_sum)/denominator\n",
        "\n",
        "    def get_AC_variance(self, sharesToSell):\n",
        "        first = 0.5 * (self.singleStepVariance) * (sharesToSell ** 2) \n",
        "        numerator = self.tau * np.sinh(self.kappa * self.liquidation_time) * np.cosh(self.kappa * (self.liquidation_time - self.tau)) \\\n",
        "               - self.liquidation_time * np.sinh(self.kappa * self.tau)\n",
        "        denominator = (np.sinh(self.kappa * self.liquidation_time) ** 2) * np.sinh(self.kappa * self.tau)\n",
        "\n",
        "        return first * numerator / denominator\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        class Info(object):\n",
        "            pass\n",
        "        \n",
        "        info = Info()\n",
        "\n",
        "        info.done = False\n",
        "\n",
        "        if self.transacting and (self.timeHorizon == 0 or abs(self.shares_remaining) < self.tolerance):\n",
        "            self.transacting = False\n",
        "            info.done = True\n",
        "            info.implementation_shortfall = self.total_shares * self.startingPrice - self.totalCapture\n",
        "            info.expected_shortfall = self.get_expected_shortfall(self.total_shares)\n",
        "            info.expected_variance = self.singleStepVariance * self.tau * self.totalSRSQ\n",
        "            info.utility = info.expected_shortfall + self.llambda * info.expected_variance\n",
        "\n",
        "        if self.k == 0:\n",
        "            info.price = self.prevImpactedPrice\n",
        "        else:\n",
        "            # Calculate the current stock price using arithmetic brownian motion\n",
        "            info.price = self.prevImpactedPrice + np.sqrt(self.singleStepVariance * self.tau) * random.normalvariate(0, 1)\n",
        "\n",
        "        # If we are transacting, the stock price is affected by the number of shares we sell. The price evolves \n",
        "        # according to the Almgren and Chriss price dynamics model. \n",
        "        if self.transacting:\n",
        "            \n",
        "            # If action is an ndarray then extract the number from the array\n",
        "            if isinstance(action, np.ndarray):\n",
        "                action = action.item()            \n",
        "\n",
        "            # Convert the action to the number of shares to sell in the current step\n",
        "            sharesToSellNow = self.shares_remaining * action\n",
        "#             sharesToSellNow = min(self.shares_remaining * action, self.shares_remaining)\n",
        "    \n",
        "            if self.timeHorizon < 2:\n",
        "                sharesToSellNow = self.shares_remaining\n",
        "\n",
        "            # Since we are not selling fractions of shares, round up the total number of shares to sell to the nearest integer. \n",
        "            info.share_to_sell_now = np.around(sharesToSellNow)\n",
        "\n",
        "            # Calculate the permanent and temporary impact on the stock price according the AC price dynamics model\n",
        "            info.currentPermanentImpact = self.permanentImpact(info.share_to_sell_now)\n",
        "            info.currentTemporaryImpact = self.temporaryImpact(info.share_to_sell_now)\n",
        "                \n",
        "            # Apply the temporary impact on the current stock price    \n",
        "            info.exec_price = info.price - info.currentTemporaryImpact\n",
        "            \n",
        "            # Calculate the current total capture\n",
        "            self.totalCapture += info.share_to_sell_now * info.exec_price\n",
        "\n",
        "            # Calculate the log return for the current step and save it in the logReturn deque\n",
        "            self.logReturns.append(np.log(info.price/self.prevPrice))\n",
        "            self.logReturns.popleft()\n",
        "            \n",
        "            # Update the number of shares remaining\n",
        "            self.shares_remaining -= info.share_to_sell_now\n",
        "            \n",
        "            # Calculate the runnig total of the squares of shares sold and shares remaining\n",
        "            self.totalSSSQ += info.share_to_sell_now ** 2\n",
        "            self.totalSRSQ += self.shares_remaining ** 2\n",
        "                                        \n",
        "            # Update the variables required for the next step\n",
        "            self.timeHorizon -= 1\n",
        "            self.prevPrice = info.price\n",
        "            self.prevImpactedPrice = info.price - info.currentPermanentImpact\n",
        "            \n",
        "            # Calculate the reward\n",
        "            currentUtility = self.compute_AC_utility(self.shares_remaining)\n",
        "            reward = (abs(self.prevUtility) - abs(currentUtility)) / abs(self.prevUtility)\n",
        "            self.prevUtility = currentUtility\n",
        "            \n",
        "            # If all the shares have been sold calculate E, V, and U, and give a positive reward.\n",
        "            if self.shares_remaining <= 0:\n",
        "                \n",
        "                # Calculate the implementation shortfall\n",
        "                info.implementation_shortfall  = self.total_shares * self.startingPrice - self.totalCapture\n",
        "                   \n",
        "                # Set the done flag to True. This indicates that we have sold all the shares\n",
        "                info.done = True\n",
        "        else:\n",
        "            reward = 0.0\n",
        "        \n",
        "        self.k += 1\n",
        "            \n",
        "        # Set the new state\n",
        "        state = np.array(list(self.logReturns) + [self.timeHorizon / self.num_n, self.shares_remaining / self.total_shares])\n",
        "\n",
        "        return (state, np.array([reward]), info.done, info)\n",
        "\n",
        "    def observation_space_dimension(self):\n",
        "        # Return the dimension of the state\n",
        "        return 8\n",
        "    \n",
        "    \n",
        "    def action_space_dimension(self):\n",
        "        # Return the dimension of the action\n",
        "        return 1\n",
        "    \n",
        "    \n",
        "    def stop_transactions(self):\n",
        "        # Stop transacting\n",
        "        self.transacting = False\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJBHcyYWA5bV"
      },
      "source": [
        "def get_env_param():\n",
        "    \n",
        "    # Create a simulation environment\n",
        "    env = MarketEnvironment()\n",
        "\n",
        "    # Set the title for the financial parameters table\n",
        "    fp_title = 'Financial Parameters'\n",
        "\n",
        "    # Get the default financial parameters from the simulation environment\n",
        "    fp_left_col = [('Annual Volatility:', ['{:.0f}%'.format(env.anv * 100)]),\n",
        "                   ('Daily Volatility:', ['{:.1f}%'.format(env.dpv * 100)])]\n",
        "    \n",
        "    fp_right_col = [('Bid-Ask Spread:', ['{:.3f}'.format(env.basp)]),\n",
        "                    ('Daily Trading Volume:', ['{:,.0f}'.format(env.dtv)])]\n",
        "\n",
        "    # Set the title for the Almgren and Chriss Model parameters table\n",
        "    acp_title = 'Almgren and Chriss Model Parameters'\n",
        "\n",
        "    # Get the default Almgren and Chriss Model Parameters from the simulation environment\n",
        "    acp_left_col = [('Total Number of Shares to Sell:', ['{:,}'.format(env.total_shares)]),\n",
        "                    ('Starting Price per Share:', ['${:.2f}'.format(env.startingPrice)]),\n",
        "                    ('Price Impact for Each 1% of Daily Volume Traded:', ['${}'.format(env.eta)]),                    \n",
        "                    ('Number of Days to Sell All the Shares:', ['{}'.format(env.liquidation_time)]),\n",
        "                    ('Number of Trades:', ['{}'.format(env.num_n)])]\n",
        "\n",
        "    acp_right_col = [('Fixed Cost of Selling per Share:', ['${:.3f}'.format(env.epsilon)]),\n",
        "                     ('Trader\\'s Risk Aversion:', ['{}'.format(env.llambda)]),\n",
        "                     ('Permanent Impact Constant:', ['{}'.format(env.gamma)]),\n",
        "                     ('Single Step Variance:', ['{:.3f}'.format(env.singleStepVariance)]),\n",
        "                     ('Time Interval between trades:', ['{}'.format(env.tau)])]\n",
        "\n",
        "    # Generate tables with the default financial and AC Model parameters\n",
        "    fp_table = generate_table(fp_left_col, fp_right_col, fp_title)\n",
        "    acp_table = generate_table(acp_left_col, acp_right_col, acp_title)\n",
        "\n",
        "    return fp_table, acp_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss3WJEFOJJRK"
      },
      "source": [
        "def generate_table(left_col, right_col, table_title):\n",
        "    \n",
        "    # Do not use column headers\n",
        "    col_headers = None\n",
        "    \n",
        "    # Generate the right table\n",
        "    if right_col:\n",
        "        # Add padding\n",
        "        if len(right_col) < len(left_col):\n",
        "            right_col += [(' ', ' ')] * (len(left_col) - len(right_col))\n",
        "        elif len(right_col) > len(left_col):\n",
        "            left_col += [(' ', ' ')] * (len(right_col) - len(left_col))\n",
        "        right_col = [('%-21s' % ('  '+k), v) for k,v in right_col]\n",
        "        \n",
        "        # Generate the right table\n",
        "        gen_stubs_right, gen_data_right = zip_longest(*right_col)\n",
        "        gen_table_right = SimpleTable(gen_data_right,\n",
        "                                          col_headers,\n",
        "                                          gen_stubs_right,\n",
        "                                          title = table_title,\n",
        "                                          txt_fmt = fmt_2cols)\n",
        "    else:\n",
        "        # If there is no right table set the right table to empty\n",
        "        gen_table_right = []\n",
        "\n",
        "    # Generate the left table  \n",
        "    gen_stubs_left, gen_data_left = zip_longest(*left_col) \n",
        "    gen_table_left = SimpleTable(gen_data_left,\n",
        "                                 col_headers,\n",
        "                                 gen_stubs_left,\n",
        "                                 title = table_title,\n",
        "                                 txt_fmt = fmt_2cols)\n",
        "\n",
        "    \n",
        "    # Merge the left and right tables to make a single table\n",
        "    gen_table_left.extend_right(gen_table_right)\n",
        "    general_table = gen_table_left\n",
        "\n",
        "    return general_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-qqHwUxJWOo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6dba1ae8-a7f5-4826-90db-d10e6bddb5a5"
      },
      "source": [
        "financial_params, ac_params = get_env_param()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in sqrt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blF-xXsaJsPf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "40eaab83-1312-40d1-a4fe-cecc7a4b1b59"
      },
      "source": [
        "financial_params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>Financial Parameters</caption>\n",
              "<tr>\n",
              "  <th>Annual Volatility:</th>  <td>12%</td> <th>  Bid-Ask Spread:    </th>     <td>0.125</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Daily Volatility:</th>  <td>0.8%</td> <th>  Daily Trading Volume:</th> <td>5,000,000</td>\n",
              "</tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.table.SimpleTable'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm9TzN_cJvlF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "0cc921ee-61de-4f1c-a486-cc9a8e49396e"
      },
      "source": [
        "ac_params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>Almgren and Chriss Model Parameters</caption>\n",
              "<tr>\n",
              "  <th>Total Number of Shares to Sell:</th>                  <td>1,000,000</td> <th>  Fixed Cost of Selling per Share:</th> <td>$0.062</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Starting Price per Share:</th>                         <td>$50.00</td>   <th>  Trader's Risk Aversion:</th>           <td>1e-06</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Price Impact for Each 1% of Daily Volume Traded:</th> <td>$2.5e-06</td>  <th>  Permanent Impact Constant:</th>        <td>0.99</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Number of Days to Sell All the Shares:</th>              <td>60</td>     <th>  Single Step Variance:</th>             <td>0.144</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Number of Trades:</th>                                   <td>60</td>     <th>  Time Interval between trades:</th>      <td>1.0</td> \n",
              "</tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.table.SimpleTable'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnAutQJYKWlF"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def hidden_init(layer):\n",
        "    fan_in = layer.weight.data.size()[0]\n",
        "    lim = 1. / np.sqrt(fan_in)\n",
        "    return (-lim, lim)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=24, fc2_units=48):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "        \"\"\"\n",
        "        super(Actor, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
        "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
        "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return F.tanh(self.fc3(x))\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    \"\"\"Critic (Value) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fcs1_units=24, fc2_units=48):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fcs1_units (int): Number of nodes in the first hidden layer\n",
        "            fc2_units (int): Number of nodes in the second hidden layer\n",
        "        \"\"\"\n",
        "        super(Critic, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
        "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, 1)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
        "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
        "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
        "        xs = F.relu(self.fcs1(state))\n",
        "        x = torch.cat((xs, action), dim=1)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sJwHnHzJxZe"
      },
      "source": [
        "import copy\n",
        "from collections import namedtuple, deque\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "BUFFER_SIZE = int(1e4)  # replay buffer size\n",
        "BATCH_SIZE = 128        # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR_ACTOR = 1e-4         # learning rate of the actor \n",
        "LR_CRITIC = 1e-3        # learning rate of the critic\n",
        "WEIGHT_DECAY = 0        # L2 weight decay\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_size, action_size, random_seed):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            random_seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(random_seed)\n",
        "\n",
        "        # Actor Network (w/ Target Network)\n",
        "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
        "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
        "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
        "\n",
        "        # Critic Network (w/ Target Network)\n",
        "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
        "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
        "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "        # Noise process\n",
        "        self.noise = OUNoise(action_size, random_seed)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
        "        # Save experience / reward\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Learn, if enough samples are available in memory\n",
        "        if len(self.memory) > BATCH_SIZE:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(experiences, GAMMA)\n",
        "\n",
        "    def act(self, state, add_noise=True):\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        state = torch.from_numpy(state).float().to(device)\n",
        "        self.actor_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action = self.actor_local(state).cpu().data.numpy()\n",
        "        self.actor_local.train()\n",
        "        if add_noise:\n",
        "            action += self.noise.sample()\n",
        "        action = (action + 1.0) / 2.0\n",
        "        return np.clip(action, 0, 1)\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.noise.reset()\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
        "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
        "        where:\n",
        "            actor_target(state) -> action\n",
        "            critic_target(state, action) -> Q-value\n",
        "        Params\n",
        "        ======\n",
        "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # ---------------------------- update critic ---------------------------- #\n",
        "        # Get predicted next-state actions and Q values from target models\n",
        "        actions_next = self.actor_target(next_states)\n",
        "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
        "        # Compute Q targets for current states (y_i)\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "        # Compute critic loss\n",
        "        Q_expected = self.critic_local(states, actions)\n",
        "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        # Minimize the loss\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # ---------------------------- update actor ---------------------------- #\n",
        "        # Compute actor loss\n",
        "        actions_pred = self.actor_local(states)\n",
        "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
        "        # Minimize the loss\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # ----------------------- update target networks ----------------------- #\n",
        "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
        "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "        Params\n",
        "        ======\n",
        "            local_model: PyTorch model (weights will be copied from)\n",
        "            target_model: PyTorch model (weights will be copied to)\n",
        "            tau (float): interpolation parameter \n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "class OUNoise:\n",
        "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
        "\n",
        "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
        "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
        "        self.mu = mu * np.ones(size)\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.seed = random.seed(seed)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
        "        self.state = x + dx\n",
        "        return self.state\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "        Params\n",
        "        ======\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "    \n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAxIGCywKUr5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "87a22c1c-3871-4761-ec0c-c9a98ed78167"
      },
      "source": [
        "\n",
        "env = MarketEnvironment()\n",
        "\n",
        "agent = Agent(state_size=env.observation_space_dimension(), action_size=env.action_space_dimension(), random_seed=0)\n",
        "\n",
        "# Set the liquidation time\n",
        "lqt = 60\n",
        "\n",
        "# Set the number of trades\n",
        "n_trades = 60\n",
        "\n",
        "# Set trader's risk aversion\n",
        "tr = 1e-6\n",
        "\n",
        "# Set the number of episodes to run the simulation\n",
        "episodes = 10000\n",
        "\n",
        "shortfall_hist = np.array([])\n",
        "shortfall_deque = deque(maxlen=100)\n",
        "\n",
        "for episode in range(episodes): \n",
        "    # Reset the enviroment\n",
        "    cur_state = env.reset(seed = episode, liquid_time = lqt, num_trades = n_trades, lamb = tr)\n",
        "\n",
        "    # set the environment to make transactions\n",
        "    env.start_transactions()\n",
        "\n",
        "    for i in range(n_trades + 1):\n",
        "      \n",
        "        # Predict the best action for the current state. \n",
        "        action = agent.act(cur_state, add_noise = True)\n",
        "        \n",
        "        # Action is performed and new state, reward, info are received. \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        # current state, action, reward, new state are stored in the experience replay\n",
        "        agent.step(cur_state, action, reward, new_state, done)\n",
        "        \n",
        "        # roll over new state\n",
        "        cur_state = new_state\n",
        "\n",
        "        if info.done:\n",
        "            shortfall_hist = np.append(shortfall_hist, info.implementation_shortfall)\n",
        "            shortfall_deque.append(info.implementation_shortfall)\n",
        "            break\n",
        "        \n",
        "    if (episode + 1) % 100 == 0: # print average shortfall over last 100 episodes\n",
        "        print('\\rEpisode [{}/{}]\\tAverage Shortfall: ${:,.2f}'.format(episode + 1, episodes, np.mean(shortfall_deque)))        \n",
        "\n",
        "print('\\nAverage Implementation Shortfall: ${:,.2f} \\n'.format(np.mean(shortfall_hist)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in sqrt\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1614: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-24485ebc3197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Action is performed and new state, reward, info are received.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# current state, action, reward, new state are stored in the experience replay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-0384a5316945>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# Calculate the current total capture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotalCapture\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_to_sell_now\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_price\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;31m# Calculate the log return for the current step and save it in the logReturn deque\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'MarketEnvironment' object has no attribute 'totalCapture'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2sOwrczOj5E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}