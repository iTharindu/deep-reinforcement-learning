{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Q_Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBjsQ3Fa0Dn5"
      },
      "source": [
        "# Deep Q-Network (DQN)\n",
        "---\n",
        "In this notebook, you will implement a DQN agent with OpenAI Gym's LunarLander-v2 environment.\n",
        "\n",
        "### 1. Import the Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hAXVVXn0Dn7"
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjrosXtYSOIy"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yu_K_FG0DoC"
      },
      "source": [
        "### 2. Instantiate the Environment and Agent\n",
        "\n",
        "Initialize the environment in the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qhKNAD80DoE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47b60ad7-533b-4076-c5e1-d946e2f1e67a"
      },
      "source": [
        "env = gym.make('CartPole-v0').unwrapped\n",
        "env.seed(0)\n",
        "print('State shape: ', env.observation_space.shape)\n",
        "print('Number of actions: ', env.action_space.n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State shape:  (4,)\n",
            "Number of actions:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yiVAdsp0DoJ"
      },
      "source": [
        "Before running the next code cell, familiarize yourself with the code in **Step 2** and **Step 3** of this notebook, along with the code in `dqn_agent.py` and `model.py`.  Once you have an understanding of how the different files work together, \n",
        "- Define a neural network architecture in `model.py` that maps states to action values.  This file is mostly empty - it's up to you to define your own deep Q-network!\n",
        "- Finish the `learn` method in the `Agent` class in `dqn_agent.py`.  The sampled batch of experience tuples is already provided for you; you need only use the local and target Q-networks to compute the loss, before taking a step towards minimizing the loss.\n",
        "\n",
        "Once you have completed the code in `dqn_agent.py` and `model.py`, run the code cell below.  (_If you end up needing to make multiple changes and get unexpected behavior, please restart the kernel and run the cells from the beginning of the notebook!_)\n",
        "\n",
        "You can find the solution files, along with saved model weights for a trained agent, in the `solution/` folder.  (_Note that there are many ways to solve this exercise, and the \"solution\" is just one way of approaching the problem, to yield a trained agent._)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpTHjiDD4CTv"
      },
      "source": [
        "import math\n",
        "\n",
        "class NoisyLinear(nn.Module):\n",
        "    \"\"\"Noisy linear module for NoisyNet.\n",
        "    \n",
        "    Attributes:\n",
        "        in_features (int): input size of linear module\n",
        "        out_features (int): output size of linear module\n",
        "        std_init (float): initial std value\n",
        "        weight_mu (nn.Parameter): mean value weight parameter\n",
        "        weight_sigma (nn.Parameter): std value weight parameter\n",
        "        bias_mu (nn.Parameter): mean value bias parameter\n",
        "        bias_sigma (nn.Parameter): std value bias parameter\n",
        "        \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, std_init: float = 0.5):\n",
        "        \"\"\"Initialization.\"\"\"\n",
        "        super(NoisyLinear, self).__init__()\n",
        "        \n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.std_init = std_init\n",
        "\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.weight_sigma = nn.Parameter(\n",
        "            torch.Tensor(out_features, in_features)\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"weight_epsilon\", torch.Tensor(out_features, in_features)\n",
        "        )\n",
        "\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.register_buffer(\"bias_epsilon\", torch.Tensor(out_features))\n",
        "\n",
        "        self.reset_parameters()\n",
        "        self.reset_noise()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"Reset trainable network parameters (factorized gaussian noise).\"\"\"\n",
        "        mu_range = 1 / math.sqrt(self.in_features)\n",
        "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.weight_sigma.data.fill_(\n",
        "            self.std_init / math.sqrt(self.in_features)\n",
        "        )\n",
        "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.bias_sigma.data.fill_(\n",
        "            self.std_init / math.sqrt(self.out_features)\n",
        "        )\n",
        "\n",
        "    def reset_noise(self):\n",
        "        \"\"\"Make new noise.\"\"\"\n",
        "        epsilon_in = self.scale_noise(self.in_features)\n",
        "        epsilon_out = self.scale_noise(self.out_features)\n",
        "\n",
        "        # outer product\n",
        "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
        "        self.bias_epsilon.copy_(epsilon_out)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\n",
        "        \n",
        "        We don't use separate statements on train / eval mode.\n",
        "        It doesn't show remarkable difference of performance.\n",
        "        \"\"\"\n",
        "        return F.linear(\n",
        "            x,\n",
        "            self.weight_mu + self.weight_sigma * self.weight_epsilon,\n",
        "            self.bias_mu + self.bias_sigma * self.bias_epsilon,\n",
        "        )\n",
        "    \n",
        "    @staticmethod\n",
        "    def scale_noise(size: int) -> torch.Tensor:\n",
        "        \"\"\"Set scale to make noise (factorized gaussian noise).\"\"\"\n",
        "        x = torch.FloatTensor(np.random.normal(loc=0.0, scale=1.0, size=size))\n",
        "\n",
        "        return x.sign().mul(x.abs().sqrt())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPm-8XD0Sp_N"
      },
      "source": [
        "class QNetwork(nn.Module):\n",
        "  \"\"\"Actor (Policy) Model.\"\"\"\n",
        "  def __init__(self, state_size, action_size, seed, fc1_unit=64, fc2_unit=64):\n",
        "    \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "    \"\"\"\n",
        "    super(QNetwork, self).__init__()\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    self.fc1 = nn.Linear(state_size, fc1_unit)\n",
        "    self.fc2 = nn.Linear(fc1_unit, fc2_unit)\n",
        "    self.fc3 = nn.Linear(fc2_unit, action_size)\n",
        "\n",
        "  def forward(self, state):\n",
        "    \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "    x = F.relu(self.fc1(state))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    return self.fc3(x)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ozeNNFe3E6R"
      },
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, in_dim: int, out_dim: int, seed):\n",
        "        \"\"\"Initialization.\"\"\"\n",
        "        super(QNetwork, self).__init__()\n",
        "\n",
        "        self.feature = nn.Linear(in_dim, 64)\n",
        "        self.noisy_layer1 = NoisyLinear(64, 64)\n",
        "        self.noisy_layer2 = NoisyLinear(64, out_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        feature = F.relu(self.feature(x))\n",
        "        hidden = F.relu(self.noisy_layer1(feature))\n",
        "        out = self.noisy_layer2(hidden)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def reset_noise(self):\n",
        "        \"\"\"Reset all noisy layers.\"\"\"\n",
        "        self.noisy_layer1.reset_noise()\n",
        "        self.noisy_layer2.reset_noise()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8Lvq6km2_pQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CakzvzkjSpzv"
      },
      "source": [
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 64         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR = 5e-4               # learning rate \n",
        "UPDATE_EVERY = 4        # how often to update the network\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xccXBsU5SpwP"
      },
      "source": [
        "class Agent():\n",
        "  \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "  def __init__(self, state_size, action_size, seed):\n",
        "    \"\"\"Initialize an Agent object.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            seed (int): random seed\n",
        "    \"\"\"\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.seed = random.seed(seed)\n",
        "\n",
        "    #Q-Networks\n",
        "    self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
        "    self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
        "    self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "    #Replay memory\n",
        "    self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "    self.t_step = 0\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    # Save experience in replay memory\n",
        "    self.memory.add(state, action, reward, next_state, done)\n",
        "    # Learn every UPDATE_EVERY time steps.\n",
        "    self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "    if self.t_step == 0:\n",
        "       # If enough samples are available in memory, get random subset and learn\n",
        "      if len(self.memory) > BATCH_SIZE:\n",
        "        experiences = self.memory.sample()\n",
        "        self.learn(experiences, GAMMA)\n",
        "\n",
        "  def act(self, state, eps = 0):\n",
        "    \"\"\"Returns actions for given state as per current policy.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state (array_like): current state\n",
        "            eps (float): epsilon, for epsilon-greedy action selection\n",
        "        \"\"\"\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "    self.qnetwork_local.eval()\n",
        "    with torch.no_grad():\n",
        "      action_values = self.qnetwork_local(state)\n",
        "    self.qnetwork_local.train()\n",
        "\n",
        "    return np.argmax(action_values.cpu().data.numpy())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def learn(self, experiences, gamma):\n",
        "    states, actions, reward, next_states, dones = experiences\n",
        "    # Get max predicted Q values (for next states) from target model\n",
        "    Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    \n",
        "    # Compute Q targets for current states\n",
        "    Q_targets = reward + (gamma * Q_targets_next * (1 - dones))\n",
        "    # Get expected Q values from local model\n",
        "    Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "    #Compute loss \n",
        "    loss = F.mse_loss(Q_expected, Q_targets)\n",
        "    #Minimize loss\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    self.qnetwork_local.reset_noise()\n",
        "    self.qnetwork_target.reset_noise()\n",
        "\n",
        "    # ------------------- update target network ------------------- #\n",
        "    self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)     \n",
        "  \n",
        "  def soft_update(self, local_model, target_model, tau):\n",
        "    \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            local_model (PyTorch model): weights will be copied from\n",
        "            target_model (PyTorch model): weights will be copied to\n",
        "            tau (float): interpolation parameter \n",
        "        \"\"\"\n",
        "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "      target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-Ur2JjChSfq"
      },
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "    \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "    \"\"\"\n",
        "    self.action_size = action_size\n",
        "    self.memory = deque(maxlen=buffer_size)  \n",
        "    self.batch_size = batch_size\n",
        "    self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "    self.seed = random.seed(seed)\n",
        "\n",
        "\n",
        "  def add(self, state, action, reward, next_state, done):\n",
        "    \"\"\"Add a new experience to memory.\"\"\"\n",
        "\n",
        "    e = self.experience(state, action, reward, next_state, done)\n",
        "    self.memory.append(e)\n",
        "\n",
        "  def sample(self):\n",
        "\n",
        "    experiences = random.sample(self.memory, k=self.batch_size)\n",
        "    states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "    actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "    rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "    next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "    dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "    return (states, actions, reward, next_states, dones)\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"Return the current size of internal memory.\"\"\"\n",
        "    return len(self.memory)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XN19TEx0DoK"
      },
      "source": [
        "agent = Agent(state_size=4, action_size=2, seed=0)\n",
        "\n",
        "# watch an untrained agent\n",
        "state = env.reset()\n",
        "for j in range(200):\n",
        "    action = agent.act(state)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    if done:\n",
        "        break \n",
        "        \n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynpwwhZp0DoO"
      },
      "source": [
        "### 3. Train the Agent with DQN\n",
        "\n",
        "Run the code cell below to train the agent from scratch.  You are welcome to amend the supplied values of the parameters in the function, to try to see if you can get better performance!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reXciNk50DoP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "outputId": "a4b6ba4f-60e7-4c91-9b9c-8da0db852bfc"
      },
      "source": [
        "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "  \"\"\"Deep Q-Learning.\n",
        "  \n",
        "  Params\n",
        "  ======\n",
        "      n_episodes (int): maximum number of training episodes\n",
        "      max_t (int): maximum number of timesteps per episode\n",
        "      eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
        "      eps_end (float): minimum value of epsilon\n",
        "      eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
        "  \"\"\"\n",
        "  scores = []\n",
        "  scores_window = deque(maxlen = 100)\n",
        "  eps = eps_start\n",
        "\n",
        "  for i_episode in range(1, n_episodes+1):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    for t in range(max_t):\n",
        "      action = agent.act(state, eps)\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      agent.step(state, action, reward, next_state, done)\n",
        "      state = next_state\n",
        "      score += reward\n",
        "      if done :\n",
        "        break\n",
        "\n",
        "    scores_window.append(score)\n",
        "    scores.append(score)\n",
        "    eps = max(eps_end, eps_decay * eps)\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "    if i_episode % 100 == 0:\n",
        "      print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "    if np.mean(scores_window)>=200.0:\n",
        "      print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
        "      torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
        "      break\n",
        "  return scores\n",
        "\n",
        "\n",
        "scores = dqn()\n",
        "\n",
        "\n",
        "\n",
        "# plot the scores\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(len(scores)), scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 100\tAverage Score: 15.14\n",
            "Episode 200\tAverage Score: 15.13\n",
            "Episode 300\tAverage Score: 14.06\n",
            "Episode 400\tAverage Score: 13.79\n",
            "Episode 500\tAverage Score: 14.61\n",
            "Episode 600\tAverage Score: 12.50\n",
            "Episode 700\tAverage Score: 11.75\n",
            "Episode 800\tAverage Score: 13.70\n",
            "Episode 900\tAverage Score: 15.01\n",
            "Episode 1000\tAverage Score: 18.20\n",
            "Episode 1100\tAverage Score: 25.05\n",
            "Episode 1200\tAverage Score: 37.10\n",
            "Episode 1300\tAverage Score: 182.78\n",
            "Episode 1310\tAverage Score: 200.89\n",
            "Environment solved in 1210 episodes!\tAverage Score: 200.89\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_7VGJGU0DoT"
      },
      "source": [
        "### 4. Watch a Smart Agent!\n",
        "\n",
        "In the next code cell, you will load the trained weights from file to watch a smart agent!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjoRzDVA0DoV"
      },
      "source": [
        "# load the weights from file\n",
        "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
        "\n",
        "for i in range(5):\n",
        "    state = env.reset()\n",
        "    for j in range(200):\n",
        "        action = agent.act(state)\n",
        "        #env.render()\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        if done:\n",
        "            break \n",
        "            \n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpxSQ9bJ0DoZ"
      },
      "source": [
        "### 5. Explore\n",
        "\n",
        "In this exercise, you have implemented a DQN agent and demonstrated how to use it to solve an OpenAI Gym environment.  To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n",
        "- Amend the various hyperparameters and network architecture to see if you can get your agent to solve the environment faster.  Once you build intuition for the hyperparameters that work well with this environment, try solving a different OpenAI Gym task with discrete actions!\n",
        "- You may like to implement some improvements such as prioritized experience replay, Double DQN, or Dueling DQN! \n",
        "- Write a blog post explaining the intuition behind the DQN algorithm and demonstrating how to use it to solve an RL environment of your choosing.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxMdQjFQzAt3"
      },
      "source": [
        "class DDQNAgent():\n",
        "  \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "  def __init__(self, state_size, action_size, seed):\n",
        "    \"\"\"Initialize an Agent object.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            seed (int): random seed\n",
        "    \"\"\"\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.seed = random.seed(seed)\n",
        "\n",
        "    #Q-Networks\n",
        "    self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
        "    self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
        "    self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "    #Replay memory\n",
        "    self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "    self.t_step = 0\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    # Save experience in replay memory\n",
        "    self.memory.add(state, action, reward, next_state, done)\n",
        "    # Learn every UPDATE_EVERY time steps.\n",
        "    self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "    if self.t_step == 0:\n",
        "       # If enough samples are available in memory, get random subset and learn\n",
        "      if len(self.memory) > BATCH_SIZE:\n",
        "        experiences = self.memory.sample()\n",
        "        self.learn(experiences, GAMMA)\n",
        "\n",
        "  def act(self, state, eps = 0):\n",
        "    \"\"\"Returns actions for given state as per current policy.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state (array_like): current state\n",
        "            eps (float): epsilon, for epsilon-greedy action selection\n",
        "        \"\"\"\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "    self.qnetwork_local.eval()\n",
        "    with torch.no_grad():\n",
        "      action_values = self.qnetwork_local(state)\n",
        "    self.qnetwork_local.train()\n",
        "\n",
        "    if random.random() > eps:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "\n",
        "  def learn(self, experiences, gamma):\n",
        "    states, actions, reward, next_states, dones = experiences\n",
        "    # Get max predicted Q values (for next states) from target model\n",
        "    Q_targets_next = self.qnetwork_target(next_states).gather(1,self.qnetwork_local(next_states).argmax(1, keepdim=True)).detach()\n",
        "    # Compute Q targets for current states\n",
        "    Q_targets = reward + (gamma * Q_targets_next * (1 - dones))\n",
        "    # Get expected Q values from local model\n",
        "    Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "    #Compute loss \n",
        "    loss = F.mse_loss(Q_expected, Q_targets)\n",
        "    #Minimize loss\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    # ------------------- update target network ------------------- #\n",
        "    self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)     \n",
        "  \n",
        "  def soft_update(self, local_model, target_model, tau):\n",
        "    \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            local_model (PyTorch model): weights will be copied from\n",
        "            target_model (PyTorch model): weights will be copied to\n",
        "            tau (float): interpolation parameter \n",
        "        \"\"\"\n",
        "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "      target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAuipiTI7JBI"
      },
      "source": [
        "agent = DDQNAgent(state_size=4, action_size=2, seed=0)\n",
        "\n",
        "# watch an untrained agent\n",
        "state = env.reset()\n",
        "for j in range(200):\n",
        "    action = agent.act(state)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    if done:\n",
        "        break \n",
        "        \n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b43J0GLq7Us_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYQZCCGX7air",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "3f3cccc4-f0fd-470c-a52d-04568ab9b3fc"
      },
      "source": [
        "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "  \"\"\"Deep Q-Learning.\n",
        "  \n",
        "  Params\n",
        "  ======\n",
        "      n_episodes (int): maximum number of training episodes\n",
        "      max_t (int): maximum number of timesteps per episode\n",
        "      eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
        "      eps_end (float): minimum value of epsilon\n",
        "      eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
        "  \"\"\"\n",
        "  scores = []\n",
        "  scores_window = deque(maxlen = 100)\n",
        "  eps = eps_start\n",
        "\n",
        "  for i_episode in range(1, n_episodes+1):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    for t in range(max_t):\n",
        "      action = agent.act(state, eps)\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      agent.step(state, action, reward, next_state, done)\n",
        "      state = next_state\n",
        "      score += reward\n",
        "      if done :\n",
        "        break\n",
        "\n",
        "    scores_window.append(score)\n",
        "    scores.append(score)\n",
        "    eps = max(eps_end, eps_decay * eps)\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "    if i_episode % 100 == 0:\n",
        "      print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "    if np.mean(scores_window)>=200.0:\n",
        "      print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
        "      torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
        "      break\n",
        "  return scores\n",
        "\n",
        "\n",
        "scores = dqn()\n",
        "\n",
        "\n",
        "\n",
        "# plot the scores\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(len(scores)), scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 100\tAverage Score: 17.28\n",
            "Episode 200\tAverage Score: 12.52\n",
            "Episode 300\tAverage Score: 11.09\n",
            "Episode 400\tAverage Score: 10.42\n",
            "Episode 500\tAverage Score: 10.03\n",
            "Episode 600\tAverage Score: 9.64\n",
            "Episode 700\tAverage Score: 9.58\n",
            "Episode 800\tAverage Score: 9.94\n",
            "Episode 900\tAverage Score: 12.33\n",
            "Episode 1000\tAverage Score: 59.08\n",
            "Episode 1100\tAverage Score: 187.51\n",
            "Episode 1139\tAverage Score: 200.33\n",
            "Environment solved in 1039 episodes!\tAverage Score: 200.33\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZxcVZn/8c/TnZUECEk6ISsJJGyC\nJNhEENQAooALyIwCMyoDzEQZFP3pjIIDo46DoiIIoyJRkE1ZBhAYEpaQBEKAQBLIvnb2PZ2tO0nv\n3c/vj7pVqa6urq7u1K2q7vq+X696dd1zz606N1W5T53lnmPujoiICEBRrgsgIiL5Q0FBRERiFBRE\nRCRGQUFERGIUFEREJKZbrgtwOAYOHOijRo3KdTFERDqV+fPn73L3kmT7OnVQGDVqFPPmzct1MURE\nOhUz29DavtCaj8ysl5m9Z2YLzWypmf0kSH/IzNaZ2YLgMS5INzO718zKzGyRmZ0ZVtlERCS5MGsK\ntcAF7n7AzLoDs83spWDfv7v70wn5LwHGBo+PAvcFf0VEJEtCqyl4xIFgs3vwSHX79GXAI8Fxc4B+\nZjYkrPKJiEhLoY4+MrNiM1sA7ASmufu7wa7bgyaiu82sZ5A2DNgUd/jmIE1ERLIk1KDg7o3uPg4Y\nDkwws9OAW4CTgbOA/sAP2vOaZjbJzOaZ2bzy8vKMl1lEpJBl5T4Fd98HzAQudvdtQRNRLfBnYEKQ\nbQswIu6w4UFa4mtNdvdSdy8tKUk6okpERDoozNFHJWbWL3jeG7gIWBHtJzAzAy4HlgSHvAB8LRiF\ndDZQ4e7bwiqfiIi0FObooyHAw2ZWTCT4POXuL5rZDDMrAQxYAHwjyD8VuBQoA6qAa0Msm4hIzryy\ndDvjR/Zj0JG9cl2UFkILCu6+CBifJP2CVvI7cGNY5RERyQe1DY18/dH5jB3Ul2nf/WSui9OC5j4S\nEcmi6LpmG/dU5bYgrVBQEBHJgXxd81JBQUREYhQUREQkRkFBRCQHLNcFaIWCgohIDqhPQUREYqOP\nANbtOshX/vQuVXUNuStQAgUFEZEs8rg6ws+nLmd22S5mrcqfedwUFEREJEZBQUQkizxpZ0L+dDsr\nKIiIZFG+djBHKSiIiGTQif/xEv/wxzltZ8zT6KCgICKSQXWNTby9Zner+z15+1HeUFAQEcmi/A4J\nCgoiIlmV5xUFBQURETlEQUFEJJtUUxARkShPGhXyJ1IoKIiI5IDjWP7csxajoCAikkXqaBYRKVD7\nqur4wxtrmt2bEH1meTS1RbzQgoKZ9TKz98xsoZktNbOfBOmjzexdMyszsyfNrEeQ3jPYLgv2jwqr\nbCIi2XDLs4u546UVzFm7J5ZWyDev1QIXuPsZwDjgYjM7G/gFcLe7jwH2AtcH+a8H9gbpdwf5REQ6\nrf01kXUSGpqaWuxL3uGce6EFBY84EGx2Dx4OXAA8HaQ/DFwePL8s2CbYf6FZPnbDiIh0XH6GgkNC\n7VMws2IzWwDsBKYBa4B97h5dZmgzMCx4PgzYBBDsrwAGJHnNSWY2z8zmlZfnz8IUIiKJktUG8rz1\nKNyg4O6N7j4OGA5MAE7OwGtOdvdSdy8tKSk57DKKiIQtXzuVk8nK6CN33wfMBM4B+plZt2DXcGBL\n8HwLMAIg2H800PpUgyIinVC+1x7CHH1UYmb9gue9gYuA5USCw98H2a4Bng+evxBsE+yf4fneTS8i\n0l5xV7V8rEF0aztLhw0BHjazYiLB5yl3f9HMlgFPmNl/Ax8ADwT5HwAeNbMyYA9wVYhlExHJqXz9\nyRtaUHD3RcD4JOlrifQvJKbXAF8KqzwiIvkgT2NBjO5oFhHJonytIUQpKIiIZFG0ozmdu7De37iX\nuoaWN76FSUFBRCQkqWoFbdUYVu/YzxW/f5ufTV2e2UK1QUFBRCRk8bWCdJuPdh+sA2DZtsoQStQ6\nBQURkSzK8y4FBQURkc7ipcXb+OHfFof6HgoKIiIhSdZUlOye3HRrDzf85X3++u7GwytUGxQURESy\nSENSRUQKVLqT/+fTZBcKCiIiOZCvFQYFBRGRkCTvU8h+OdpDQUFEJGTxzUP5ugxnlIKCiEg+y3IM\nUVAQEckiNR+JiBSopKustfdFsjw0SUFBRCQH8nVhSQUFEZGQJFtu83DuaM4GBQURkSyKBgAzS/vm\ntmxSUBARCUnSPoV8qhYkoaAgIhK2JDWCtPsUNCRVRKQrS+8qn6uWpdCCgpmNMLOZZrbMzJaa2beD\n9B+b2RYzWxA8Lo075hYzKzOzlWb2mbDKJiKSK3laQYjpFuJrNwDfc/f3zexIYL6ZTQv23e3ud8Zn\nNrNTgauADwFDgdfM7ER3bwyxjCIioUk7AKTK11XuU3D3be7+fvB8P7AcGJbikMuAJ9y91t3XAWXA\nhLDKJyKSLfFDU5Nd/9s7H1KY9zhkpU/BzEYB44F3g6RvmtkiM3vQzI4J0oYBm+IO20ySIGJmk8xs\nnpnNKy8vD7HUIiKZEX/Rz8TMqWGOYAo9KJhZX+AZ4DvuXgncB5wAjAO2Ab9uz+u5+2R3L3X30pKS\nkoyXV0QkU5Ldh9Da1BeNTc7Ppy5nZ2VNm68bZn9DqEHBzLoTCQh/cfdnAdx9h7s3unsT8EcONRFt\nAUbEHT48SBMR6ZRS/aKP3+XuzFm7m/tnreX7zyxKmjG+yahTNh+ZmQEPAMvd/a649CFx2b4ILAme\nvwBcZWY9zWw0MBZ4L6zyiYhkS7M+hbjreXxNoinYUd/YFBzTXHV9Y1zejBcxJszRR+cCXwUWm9mC\nIO2HwNVmNo5I/FsPfB3A3Zea2VPAMiIjl27UyCMR6WqiQcES0hLnSYq/7jc0NrH7QF3cvvCiQmhB\nwd1nk3ww1dQUx9wO3B5WmURE8kWz5qM2LvKf/NXrbNlXfSh/Z+5oFhEpVOkOP005IsloFhDCpqAg\nIhKy9gxJfatsN8u3VaasPdTUh9eyrqAgIhK2Npp7EndPenTeocQkx37kv1/LRKmSUlAQEcmQxKGi\n0U7VtroAWh5nzFq9q9X8jSEOP1JQEBHJkMSmoaR9CsmajxK2N+6p4g9vrMlUsdpFQUFEJENa+/0e\nHwiS9hXk0cI7CgoiIhnS2p3Gs8t2sa+qLiHvoef7qut4a03rzUXZpKAgItJBjU3Opj1Vbeb7wxtr\nuPahuUDy5qOfTV3Bfa+3r7korGGqCgoiIh1017SVfPyXM2OBYef+2uYZ4gLA8m2ViUnpaWU9hUfe\nXt/eV0qLgoKISAfNLtsNQPmBSDD4t/9d2GrepqaWaVMXb2/7TVqJIvWN4XREKCiIiHRU0BYU/TEf\nP2ndrgPNaw0NQVTI1AynjcmiTAYoKIiIdFBsJookCyeUJtxgFr21IFO/7+tDuldBQUFEpIOSzXia\nLQ2NqimIiOSV6D0HyVZYS5rfPWMznDaopiAikl8O1RTSiwqRzuH2XcxbmxivQR3NIiL5JRYU0qwp\n1DU2tbum0Fr+4wYc0b4XSpOCgohI4IWFWxl18xSq6hrSyh+9Xm/YXcXmvVVJ5j5qnlDf0P5+gMZW\nosJXzzmu3a+VjjCX4xQR6VTunrYKgG0VNZxQ0rfN/NHhpTf+9X0Axo3olzL/3qq6do8+amql76B7\nUTi/6VVTEBHJkLaakS749Rvtbj5auLkiaXq34nDGPCkoiIh0UJhrJbele3EnqymY2Qgzm2lmy8xs\nqZl9O0jvb2bTzGx18PeYIN3M7F4zKzOzRWZ2ZlhlExFJJd3f4E0JUWFnZW0rOQ/J1B3N3Yo6X02h\nAfieu58KnA3caGanAjcD0919LDA92Aa4BBgbPCYB94VYNhGRw5Z4eU9n5tJMVS6KO1tQcPdt7v5+\n8Hw/sBwYBlwGPBxkexi4PHh+GfCIR8wB+pnZkLDKJyJyuDryqz8TFYXPnzE06dQamZCVPgUzGwWM\nB94FBrv7tmDXdmBw8HwYsCnusM1BWuJrTTKzeWY2r7y8PLQyi0jhae9FviPX95ue+KADRzU3tF+v\nw36N1oQeFMysL/AM8B13r4zf55FPoF3/ru4+2d1L3b20pKQkgyUVEWmnNq5eyWJMeeKaCx1QHFIt\nAUIOCmbWnUhA+Iu7Pxsk74g2CwV/dwbpW4ARcYcPD9JERHLu1aXb2VFZw/pdB5m9OrJ0Zq4GH4XV\nnwAh3rxmkQavB4Dl7n5X3K4XgGuAO4K/z8elf9PMngA+ClTENTOJiGRNYnt9Y5Mz6dH5jBpwBOt3\nR1ZZW3/HZ9vV3HREj2Kq6hrbzpiGohBrCmkHBTM7Dxjr7n82sxKgr7uvS3HIucBXgcVmtiBI+yGR\nYPCUmV0PbAC+HOybClwKlAFVwLXtOhMRkQyrbWjk3bV7WBYspRkNCFFthYR5G/bGnmeyySfnNQUz\n+xFQCpwE/BnoDjxG5MKflLvPpvXhvhcmye/AjemUR0QkTNEawEm3vtxGvvRfsyiDF/Iwg0K6fQpf\nBL4AHARw963AkWEVSkQk25ZsqeBAbaR5J91rfeLNa6lUVNd3oFTJ5UPzUZ27u5k5gJn1Ca1EIiJZ\nVl3XyOf+Z3ZsO91rfa6muQhphgsg/ZrCU2Z2P5Ebyv4FeA34Y3jFEhHJnrqEKa0jK6SlvuJnarqK\njgizppBWUHD3O4GniQwvPQn4T3f/n9BKJSKSRYnrHjjRVdJal8nlMM8/qX33XOW0+cjMioHX3P18\nYFpoJRERyZHE63uTOw1NqRfE2by3OmO1hQtOGczMlenP0JDTjmZ3bwSazOzo0EohIpJDiRd397Zr\nCuff+TpbK2rSfo+SI3s2Pz6oHXRkWc1MjmRKlG5H8wEi9xtMIxiBBODuN4VSKhGRLEq8/De5s+vA\n4U9HEe+kwUc2m+LiWxeO5edXfJg+PYt5bsHWWPqVpSN4ct6mZC8Rkw/TXDwL3AbMAubHPUREOr3E\noaXucM2D72X0Pe65alyz7e5FRRx7dC+O7NU9lnbasKO4asKIxENj/vm80QB89Pj+GS1bvLRqCu7+\nsJn1AE4Mkla6e+YG3YqI5FJCVeG6h+ayMwMT18U75ogezbbjl9OMPvvw8H6MH3kM6+/4LKNuntIs\n/79/5iRuPH8Mt37u1IyWK1G6dzRPJLL2wXoi5R9hZte4+6zwiiYikh2JzUeZDgjQsh8gfjnNdLqr\nGzM42imVdPsUfg182t1XApjZicDjwEfCKpiISLa0587kTOkeV1Po1S0SIPr2bP2SnK0yphsUukcD\nAoC7rwqmxRYR6fRycR9afE3hi+OHsXN/LdeeO6rV/ONG9MtCqdLvaJ5nZn8ys4nB44/AvDALJiKS\nLWHFhH/5+OhW98X3KXQrLuLG88dwRI/kv9Pn3/opJp40KOPlSybdoHADsAy4KXgsC9JERDq9ppDa\n6z95YusX8h5pTmA0on9vBvTt2XbGDEm3+agbcE90sZzgLufslVJEJERhNR/16t76hb9bmkHhze9f\nkKnipCXdmsJ0oHfcdm8ik+KJiHR6YXXi9ujW/BK74qcXx57HdzTnk3SDQi93PxDdCJ63/95sEZE8\nFFZQ6NmtuNl2r+6HtrsXhTj/9WFIt1QHzezM6IaZlQLV4RRJRCS7wroFoGe31i+x6cxfdMlpx2ay\nOGlJt0/hO8D/mll0go4hwJXhFElEJLvSne302xeO5Z7pq9vM9/GxA3lz9S66B0EhVXBozZqfXUqI\n8961KmVQMLOzgE3uPtfMTga+DlwBvAysy0L5RERCl4mawuCjerKjMnIn9OSvlrJzf03sot67x6Fm\no1/83en87YMtbb5emNNjp9JW+LofqAuenwP8EPgdsBeYHGK5RERCN3f9Hr724HvUN7ZcO+GWS05m\nQJ/m8xWlWtymW1wfQe8exRw3oE9saorecX0JV541kicmnXO4RQ9NW0Gh2N33BM+vBCa7+zPufhsw\nJtWBZvagme00syVxaT82sy1mtiB4XBq37xYzKzOzlWb2mY6ekIhIur711w+Ytaqcnftbrotw+rCj\nOW1Y82Vk4keRPnBNKdefd+jmtGRzE9XUR4JNfFDId20GBTOLNjFdCMyI29dWf8RDwMVJ0u9293HB\nYyqAmZ0KXAV8KDjm98G9ECIioaltaASS1wCKi4w3VjVfDe3Ks0bGnl94ymA+ferg2PZdXz6DH3/+\nVL7+yeNjadFhp6cMOSqj5Q5TWxf2x4E3zGwXkdFGbwKY2RigItWB7j7LzEalWY7LgCfcvRZYZ2Zl\nwATgnTSPFxFpt9qGyC/5hiSrrCVr009cPW3MoL4A/M/V4/nYmIF8bMzAZvuPG9CHP197Fh8dHd76\nB5mWMii4++1mNp3IaKNX/VAXfRHwrQ6+5zfN7GtE5k76nrvvBYYBc+LybA7SWjCzScAkgJEjRybL\nIiKSlmhQSNanUFRkjB7Yh3W7IotNJrs7eUDfnqy/47Mp3+P8LM1ZlCnprNE8x93/5u7xy3Cucvf3\nO/B+9wEnAOOAbUSm5G4Xd5/s7qXuXlpSUtKBIoiIRET7AdbtPthin7sz5abzmPlvEwE4b0zkevP1\nTx7fYhW1riTd+xQywt13RJ8HM62+GGxuAeLXoBsepImIhO6XL69skdbYBEf06Mbogd14YtLZnDE8\nMnX1LZecku3iZVVW77M2syFxm18EoiOTXgCuMrOeZjYaGAtkdoFUEZF2iB9NdPbxA5rda9CVhVZT\nMLPHgYnAQDPbDPwImGhm44hMX76eyM1wuPtSM3uKyJTcDcCN7t4YVtlERNqSi9XY8kFoQcHdr06S\n/ECK/LcDt4dVHhGR9mjI0prI+SY/p+kTEcmx4wf2yXURckJBQUQkwVmjjmFE/8JcHUBBQUQkQd+e\nWR2YmVcUFESkYLU2EekVZw7PbkHySOGGQxEpeEVmLUYZPf2Ncygd1Xmmpcg01RREpGAlW/0sxezY\nBUFBQUQKVnHSCFDYUUFBQUS6HHfn3umr2bSnKmW+ZH0KqimIiOSxldv3s3DTvnYds2VfNXdNW8W1\nD81NmS9p81G73qnrUUeziOS1z/xmFkCbU1THi/Yd19Snni0nV+sg5zPVFESkYCVbcc0KvP1INQUR\n6XLamsuupr6Rv32wBU+SsbBDgoKCiBSgu6et4v5Za5PuK/CKgpqPRKTraevCvnN/bevHFnhdQUFB\nRApOqrUSVFMQESkwBbpUQloUFESk4DQpKrRKQUFECo6aj1qnoCAiBacxRU1BHc0iIgUmVeuRagoh\nMbMHzWynmS2JS+tvZtPMbHXw95gg3czsXjMrM7NFZnZmWOUSEVHzUevCrCk8BFyckHYzMN3dxwLT\ng22AS4CxwWMScF+I5RKRApcqKBS60IKCu88C9iQkXwY8HDx/GLg8Lv0Rj5gD9DOzIWGVTUQKz4bd\nB6lraALaaD5Sn0JWDXb3bcHz7cDg4PkwYFNcvs1BWgtmNsnM5pnZvPLy8vBKKiJdRmVNPZ/81evc\n8uxiIPWQVDUf5YhHZqJqdx3O3Se7e6m7l5aUlIRQMhHp7BJbh6pqI1Nozy6L/JBM2acQWqk6h2wH\nhR3RZqHg784gfQswIi7f8CBNRCQtW/dVs+tAZE4jb+P3ZsohqQUeFbIdFF4ArgmeXwM8H5f+tWAU\n0tlARVwzk4hImz52xwxK//s1oO2ps9XP3LrQps42s8eBicBAM9sM/Ai4A3jKzK4HNgBfDrJPBS4F\nyoAq4NqwyiUiXV9b1/zUNYnCriqEFhTc/epWdl2YJK8DN4ZVFhEpLMkWz0mXmo9ERLqYxJCQWDNI\nNey0wGOCgoKIdD1tVhRSXPkLfY1mBQUR6YJS1wxSXfYLOyQoKIhIF5RYU2hriKocoqAgIl1Om61H\nKZuPMlqUTkdBQUS6nBY1hXZUFDT3kYhIF5PYXBSd1iJ6wU85+qiwY4KCgoh0Pa3VFKLBotAv/Kko\nKIhIl9NW85H6FFoX2h3NIiK5cNerK3lxcfOp09pz81qhU1AQkS7l3hllLdLa1dFc4FUFNR+JSJfX\noqM5VfNRNgqUxxQURKTLa8+tawVeUVBQEJGurz2zphZ6f4OCgoh0eS1HH+k+hdYoKIhIp1Jd18j8\nDXvadUw0JmyvrOHqyXMKvC6QmoKCiHQq//b0Qv7uvne47HdvUVPfmNYxTXFVhXfW7lZHcwoKCiLS\nqSzdUgHAwk37eGft7rSOaddCbAUeFRQURKTLa/LEm9dap45mEZFOpCM3l7Xv5rV2v3yXoqAgIp1K\ne4aXdkSBx4TcTHNhZuuB/UAj0ODupWbWH3gSGAWsB77s7ntzUT4R6STSjA+JzUdah611uawpnO/u\n49y9NNi+GZju7mOB6cG2iMhha8+iO5r7KH9cBjwcPH8YuDyHZRGRPNXsop3m9bs9NYXCDgm5CwoO\nvGpm881sUpA22N2j891uBwYnO9DMJpnZPDObV15eno2yiki+SrMdKDFbqn6JAq8o5Gzq7PPcfYuZ\nDQKmmdmK+J3u7maW9FNz98nAZIDS0lI1DYpIm9rVfFTgdYWc1BTcfUvwdyfwN2ACsMPMhgAEf3fm\nomwi0omkef1OrBkkNifJIVkPCmbWx8yOjD4HPg0sAV4ArgmyXQM8n+2yiUgn08a1PdoU1LL5KNVB\nh1Ogzi8XNYXBwGwzWwi8B0xx95eBO4CLzGw18KlgO6+de8cM/vDGmlwXQ0Ta0NSUfk1BfQpZ5u5r\ngTOSpO8GLsxWOarrGjGDXt2LO/waW/ZVc8dLK/jGJ0/IYMlEJNNa1BRS5C3wmJBXQ1Kz6pT/fJlP\n/mpmroshIu3UkYt2y47mVDWFwg4LBRsUAHZU1ua6CCKSAW1NfZG4P/Xoo8JW0EHhcIQ9/4qINJfq\n/1xb/x3b03xU6BQUOmj6co2YFcmmZBf+aIdxWxf5xI5ldTS3riCDwvMLthz2a/zzI/MyUBKRwvKJ\nX87kuofmppV3e0VNs2U3k13Go4OKWqtFbNpTHexvnq6b11qXqzuac6qkb8+k6RVV9fzjA3O456rx\nnFDSF4BH31nPws0V7D5Qy7984nj69e7BD55ZlMXSinQdG/dUsXFPVVp5J945k5r6pth25MLf/IL9\nhzfWcNGpg2lKcZF3d01z0Q4FGRSO6dMjafq05TtYsqWS384o4+4rxwFw2/NLY/sXba7gQ8OOZnGw\nHKBIV/XK0u24OxefNiRjr7nrQPsGdsQHBEheU5i/YW+wr/WLfHV9o6bOboeCbD4aM6hv0vTor4fW\nfikc0bOYOWtarglbUV3frvdvanIenL2OlxZvazuzSA58/dH5fOOx9zP6mne+svKwjo9d15P8/0zV\nHLS/pqFFFFCfQusKMih0Ly7im+ePwexQIKiqa2DdroMAFLXyrejZrZi6xqYW6Tsqa9r1/o/P3ch/\nvbiMG/7yPtsqqlvsX7frIDX1je16TZF8d7gX21htIMn1PFVQqKyub1lTUFWhVQUZFAB69yjGndhF\nftIj8/n965EpK4qCL+/LS7Y3O6a1i//T8zfz6tJI3vL9tfzn80uoa2jC3alPEkS27Tv0Og/OXseM\nFTti23UNTZx/5+t86/EPUpZ/894q/uv/ltGYqjFVpAPCGm49+Kheh3V8qmKlaj6qrGlQR3M7FGxQ\n6NktcurRdsvZZbti+6Jfim88Nr/ZMftrGpK+1uRZa5n0aCTv7VOW8cg7G3ht+Q4em7OBsf/xEuX7\nm7elxtc2/vjmOq576NBIptqGSA1h1qrUa0V896mFPPjWOhZu3pcyn0h7JasNZ8KRvboDMKxf78N7\noSTX7FS/jSprWtYU4re7FTV/wUJvPirIjmY4NOfRGT95le9edGKzfY3uzX69t0d9Y+TLdttzSxga\nfPm3VVRTcuShEU91Da3/p4sGqdoUearrGlmwKRIMdBOdZFpiB2+mRL/33Yo7dtVNWVNIsXN/TUPs\nR2BUQ1wUOap3d/YcrIttF3hMKNyaQnyzy13TVjXbN3v1rma/3jti98E6KmsiHdBFZjz3wRYem7MB\nSP5LbGFwkY/vS4jWGhJ958kPWgSWhsYmbn5mUaxfpCNq6htTBiwpDLUh9WdFv1vpXHSTXeRTNRGl\nrClU17cIKPH//xMqCpr7KNcFyJXqFF/87e3sOI6qbWhkStyIog27I+Ox6xqb+M6TC7j1uSWR7SQX\n3i/d/07sNWJlrEtexrfLDo2AaghqJiu27+eJuZv41uORESMV1fX8+a11adcklm+r5OTbXua0H7/C\nk3M3pnWMdE2h1RQaI9/ndLrBojXueCm/ym2MPkrse2toOnSOiUGgsENCIQeFJBfcKTedx3ljBnb4\nNT937+yk6Vf8/u3Y88t+Oztp01A0UMT/hxz3X9P40H++zM79zYNUfdwX+srJcwBYtq0SgCVbKjlY\n28Btzy3hJ/+3jNG3TOXZ9ze3WfZL7nkzVo4fPLOYUTdPYdaqcj7y02ls3F3FvdNXc8Gdr7No8z72\nHqxj4aZ9jLp5Ci8u2hp7jSfnbuTDP34lZef3topqVu/YD8D05Ts47xczqKiqp6K6ngWb9jF51ho+\ne++bLYLZsq2VfOSn0yjbub/Fa7o7s1fvCr0pbc7a3a3W3rqSmpDOMfodb0ijzyJZGVJ9uql+5P3i\n5RXc+Nfmw2ujdzpD87UWzhh+dMH3KRRsUPjYCQNapJ04+EiG9uv4CInVOw+0mWfh5gr+b+HWpPte\nW7aDf/zTu83SDtY18ticjdQ3NrFzfw07K2ta/JJbW36A7z996C7rv7vvbZZuPXSD3XefWhj7D3mg\ntoFNe6p4c3U5o26ewsm3vRTrn0j0rcc/YPfBOn47czV3TVvF2l0H+cJv32L8T6dx56uRMeff/OsH\nnHzbS6zfdZBbnl1MZU0D63cfjI3Uqq5rpKK6Prb9sTtmcNHdswC4f9ZaNu+tZsbKHfzLw/O4/Hdv\n8bOpK1i6tTIW5DburuLGv77PD55ZxO6DdVz70FzqGprYV1XHzuA1/3feZr7ywLu8EPy7RoNoRVV9\ni4t49FiIBJPEQQDJHKxtYMGmfVw1eQ4/m7K82XtE1dRHznPn/hoO1jZw41/eZ8Pu9JryEl8r3Xx7\nDtYlHdJ8uGo7WFOoqW+koir5PTtNTc7Wikj56+MuwvWNTeyNa88HIt/xJD/aWgv6ew7WMW15x/oA\nAe69enzs+fPfPK/gm48KtiGBltEAAA4LSURBVKP5o8cPYMpN5/G1B95jd/Cl7F5cxK4DdW0cmRkT\nRvfnvXV7mqW1Np/SvdNXs2RLBTNWJJ+E74Jfv9Fse8X2lr+mT7z1JW773Kn89MVlzdJr6pu4/Hdv\nJX3d6E15T81rWdN4c/Wh0Vo19U1MvPP12PaFQXk+f8ZQ9hys5a2guetPXyuNNQFMWbQtdv7LtlYy\nd0Pzf4sFm/YxdtCRfCJhzYtNe6o55+fTY58ZQN+eka/x5r3VPDl3Iz94ZjE3X3Iyd7y0AoAnJ53N\n9BU72VFZw/MLIoHjwX8qjfUbXXr6sXz3ohO54bH3ue1zpzJn7W7eW7eHK84cztUTRnDuL2awL7jY\nLdxcwYrtlVz8mze5esJIxgzqS2V1PVMWb6Ms+FHw5dLhTFm8jZ7di7jry+PYsq+av72/mX+dOIaZ\nK3fS2ORcdOpgzIylWyv47L2zueOK0+l3RHd6dS/mE2NL2FNVx8Nvr+fo3t05b+xANuyu4uvBCLdH\nr5/A1n3V/OCZxUDkZsypN32c4iLj6fmbqGtoorioiKvOGnFoOUqHoiJj7vo93Dt9NVv2VTNhVH8G\nH9WL6vpGrj13FMce1YsPNu3jifcONR9+5U/vctao/vTpWcwJg/qyeHMFN0w8ge7Fkd+TDY1NFBcZ\nTQ43PDafmSvLWfHTi5m3fi/TV+zg7bLd/M8/jOeSe96M1SDrG5tobHIM+N5TC3lh4VbW/OxSiouM\n11fu5J/+PJdbLjm5xXdu14E6+vbsRvei5r9lz/zpNC457dgW+dN17piB3P7F0xh69GGOiuoirDOP\nXiktLfV58w6vQ3jvwTrG/3QaI/sfwazvn88tzy7m8fc28ud/OouPjRnAH2et5c5XV/Hm98/n4788\ndIE6+dgjm118//0zJ/GrV1Zy7FG9+MSJA5NeSKO+e9GJXHXWCCb8bPphlV2kkP3Tx0bx0NvrW93f\nv08P3r75AjbtqeLYo3vx5NxNvLGqnB9eegonDT6S26cux4BbP3dq1sqcL8xsvruXJt1X6EEh0YHa\nBmau2MnnzxjaYt/K7fs5WNfAzsoaPjZmIDV1jUz42XTuuWocn/vwUB6bs4GJJ5Vw3IA+sWNOuvWl\nZn0Ix5f0Ycb3JgIwc8VOlm2r5F8nnsDoW6bG8pxz/ADeWRv5df2Vs0dywcmDWL5tP79KmCbg8nFD\neW7Boaao8SP78cHG5E1BPYqLYqOeLj39WKYu3s5l44Zy3bmj+fenF7JqxwHOGnUMI/v34ZLTjk1r\nFtjLxg2N/fKOuvDkQUxPqNFcd+5omtxb/Ae+fNxQDtQ28tphVP0lInJ3fv6+Xqb9/IrTuXrCSEbd\nPCWW9vkzhsaaZm/97ClcedaI2L0R0pyCQojcPa02SHePVeFb2x9lZrg7TQ7FcfndndqGJnoUF2EW\nydfU5LEmguj2jBU72XOwji+VDueVpdsxMz4dN5Nk9DXjy15T39hsveqmJueuaav4/BlDOXFwZK6o\nJj80fC96Lu5OdX0jv51RxpVnjeC4AX2oqW+kZ7ciVmzfz9TF2/h/nzqRoiKjsqae30xbzQ0TT6Bv\nz2707hF5v3fW7Oa6h+ZyyenHUmzGl88awRnD+1Fkkc7F37y2iitLR2IGT87dxFfPOY7731jLtz81\nlhcWbGH0wL58sHEvE0b358zjjqFbkdEQNE+UH6jl9zPX8J1PjeUPb6zhnz9+PIOP6kVlTT2LNkWa\ngqrrGrnoQ4M5afCR7Kuq5+F31nPRqYM5fmDkvHv3KKaqroEexUX8etoqPnv6EMYM6kuP4iIceHXp\ndvbXNPD5M4ZiFhkDf0T3Yprc+eUrKxk1oA9fKh3O7VOWM2ZQX9aWH+S680axeW81T83bxMj+RzB+\n5DEs31bJgo37qKyp55HrJvDCwq1MXbyd4cf05uLTjmXp1kouPHkQT8zdxA0TT+C1ZTvoVmx84Yyh\nNDn8dkYZE08q4fiSPvTuXowTGXlz/xtr+IePjmRg357UNzZxdO/u1DY0YQZvrtrFna+u5NLTh3Dd\neaN59v3NjBnUl7NHD6C2oYle3YtYsGkf767bwzXnjKJHtyKMSEfwX9/dyOnDjua3M8s4qnd37v7y\nOH796kq+VDqc4wb0Yc/BOr79xAd84sQSvvGJE/Dgu9fQ2MTWfTU8PncjN0w8gW5Fxp2vrOK680Yx\n6MhezFixg5eXbOfM447hq2cfF/ve1TU2cc9rq7nwlEF85Lj+ALy0eBtNDp/50GC6FRfx/sa9zF69\ni29dMKbg+wZS6VRBwcwuBu4BioE/ufsdreXNh6AgItLZpAoKeTX6yMyKgd8BlwCnAlebWeE1+ImI\n5EheBQVgAlDm7mvdvQ54Argsx2USESkY+RYUhgGb4rY3B2kiIpIF+RYU2mRmk8xsnpnNKy9PPZOo\niIi0T74FhS3AiLjt4UFajLtPdvdSdy8tKSnJauFERLq6fAsKc4GxZjbazHoAVwEv5LhMIiIFI6+m\nuXD3BjP7JvAKkSGpD7r70hwXS0SkYORVUABw96nA1DYziohIxuXdzWvtYWblwIYOHj4Q2NVmrs6n\nK55XVzwn6Jrn1RXPCbreeR3n7kk7ZTt1UDgcZjavtTv6OrOueF5d8Zyga55XVzwn6LrnlUy+dTSL\niEgOKSiIiEhMIQeFybkuQEi64nl1xXOCrnleXfGcoOueVwsF26cgIiItFXJNQUREEigoiIhITEEG\nBTO72MxWmlmZmd2c6/Kky8xGmNlMM1tmZkvN7NtBen8zm2Zmq4O/xwTpZmb3Bue5yMzOzO0ZtM7M\nis3sAzN7MdgebWbvBmV/Mpj2BDPrGWyXBftH5bLcqZhZPzN72sxWmNlyMzuni3xW/y/4/i0xs8fN\nrFdn+7zM7EEz22lmS+LS2v3ZmNk1Qf7VZnZNLs4l0wouKHTyhXwagO+5+6nA2cCNQdlvBqa7+1hg\nerANkXMcGzwmAfdlv8hp+zawPG77F8Dd7j4G2AtcH6RfD+wN0u8O8uWre4CX3f1k4Awi59epPysz\nGwbcBJS6+2lEpqO5is73eT0EXJyQ1q7Pxsz6Az8CPkpkLZgfRQNJpxZZO7hwHsA5wCtx27cAt+S6\nXB08l+eBi4CVwJAgbQiwMnh+P3B1XP5Yvnx6EJkNdzpwAfAiYETuHu2W+JkRmRfrnOB5tyCf5foc\nkpzT0cC6xLJ1gc8quuZJ/+Df/0XgM53x8wJGAUs6+tkAVwP3x6U3y9dZHwVXU6CLLOQTVMPHA+8C\ng919W7BrOzA4eN5ZzvU3wPeBpmB7ALDP3RuC7fhyx84p2F8R5M83o4Fy4M9Bs9ifzKwPnfyzcvct\nwJ3ARmAbkX//+XT+zwva/9l0is+svQoxKHR6ZtYXeAb4jrtXxu/zyE+WTjPO2Mw+B+x09/m5LkuG\ndQPOBO5z9/HAQQ41RwCd77MCCJpHLiMS9IYCfWjZDNPpdcbPJlMKMSi0uZBPPjOz7kQCwl/c/dkg\neYeZDQn2DwF2Bumd4VzPBb5gZuuJrMl9AZG2+H5mFp3FN77csXMK9h8N7M5mgdO0Gdjs7u8G208T\nCRKd+bMC+BSwzt3L3b0eeJbIZ9jZPy9o/2fTWT6zdinEoNBpF/IxMwMeAJa7+11xu14AoiMfriHS\n1xBN/1oweuJsoCKuepwX3P0Wdx/u7qOIfBYz3P0fgZnA3wfZEs8peq5/H+TPu1907r4d2GRmJwVJ\nFwLL6MSfVWAjcLaZHRF8H6Pn1ak/r0B7P5tXgE+b2TFBDerTQVrnlutOjVw8gEuBVcAa4D9yXZ52\nlPs8IlXaRcCC4HEpkTba6cBq4DWgf5DfiIy0WgMsJjJiJOfnkeL8JgIvBs+PB94DyoD/BXoG6b2C\n7bJg//G5LneK8xkHzAs+r+eAY7rCZwX8BFgBLAEeBXp2ts8LeJxIn0g9kVrd9R35bIDrgnMrA67N\n9Xll4qFpLkREJKYQm49ERKQVCgoiIhKjoCAiIjEKCiIiEqOgICIiMQoKUpDMrNHMFsQ9Us6Wa2bf\nMLOvZeB915vZwA4c9xkz+0kwk+dLh1sOkdZ0azuLSJdU7e7j0s3s7n8IszBp+DiRG8Q+DszOcVmk\nC1NNQSRO8Ev+l2a22MzeM7MxQfqPzezfguc3WWRNi0Vm9kSQ1t/MngvS5pjZh4P0AWb2arD+wJ+I\n3AgVfa+vBO+xwMzuD6Z1TyzPlWa2gMh01b8B/ghca2ad4i586XwUFKRQ9U5oProybl+Fu58O/JbI\nhTjRzcB4d/8w8I0g7SfAB0HaD4FHgvQfAbPd/UPA34CRAGZ2CnAlcG5QY2kE/jHxjdz9SSKz4S4J\nyrQ4eO8vHM7Ji7RGzUdSqFI1Hz0e9/fuJPsXAX8xs+eITF8BkSlI/g7A3WcENYSjgE8AVwTpU8xs\nb5D/QuAjwNzIFEL05tAEbIlOBNYGz/u4+/40zk+kQxQURFryVp5HfZbIxf7zwH+Y2ekdeA8DHnb3\nW1JmMpsHDAS6mdkyYEjQnPQtd3+zA+8rkpKaj0RaujLu7zvxO8ysCBjh7jOBHxCZCrov8CZB84+Z\nTQR2eWSti1nAPwTplxCZFA8iE6/9vZkNCvb1N7PjEgvi7qXAFCJrGPySyASO4xQQJCyqKUih6h38\n4o562d2jw1KPMbNFQC2RJRfjFQOPmdnRRH7t3+vu+8zsx8CDwXFVHJqC+SfA42a2FHibyNTTuPsy\nM7sVeDUINPXAjcCGJGU9k0hH878CdyXZL5IxmiVVJE6w2E+pu+/KdVlEckHNRyIiEqOagoiIxKim\nICIiMQoKIiISo6AgIiIxCgoiIhKjoCAiIjH/H4acnR8POi+3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzNiOGB37ckP"
      },
      "source": [
        "class QNetwork(nn.Module):\n",
        "  \"\"\"Actor (Policy) Model.\"\"\"\n",
        "  def __init__(self, state_size, action_size, seed, fc1_unit=64, fc2_unit=64):\n",
        "    \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_unit (int): Number of nodes in first hidden layer\n",
        "            fc2_unit (int): Number of nodes in second hidden layer\n",
        "    \"\"\"\n",
        "    super(QNetwork, self).__init__()\n",
        "    self.feature_layer = nn.Sequential(\n",
        "        nn.Linear(state_size, fc1_unit), \n",
        "        nn.ReLU(),\n",
        "    )       \n",
        "    # set advantage layer\n",
        "    self.advantage_layer = nn.Sequential(\n",
        "        nn.Linear(fc1_unit, fc2_unit),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(fc2_unit, action_size),\n",
        "    )\n",
        "    # set value layer\n",
        "    self.value_layer = nn.Sequential(            \n",
        "        nn.Linear(fc1_unit, fc2_unit),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(fc2_unit, 1),\n",
        "    )\n",
        "  def forward(self, state : torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Forward method implementation.\"\"\"\n",
        "    feature = self.feature_layer(state)\n",
        "    \n",
        "    value = self.value_layer(feature)\n",
        "    advantage = self.advantage_layer(feature)\n",
        "\n",
        "    q = value + advantage - advantage.mean(dim=-1, keepdim=True)\n",
        "    \n",
        "    return q\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfy7DKP91iV_"
      },
      "source": [
        "class Agent():\n",
        "  \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "  def __init__(self, state_size, action_size, seed):\n",
        "    \"\"\"Initialize an Agent object.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            seed (int): random seed\n",
        "    \"\"\"\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.seed = random.seed(seed)\n",
        "\n",
        "    #Q-Networks\n",
        "    self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
        "    self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
        "    self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "    #Replay memory\n",
        "    self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "    self.t_step = 0\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    # Save experience in replay memory\n",
        "    self.memory.add(state, action, reward, next_state, done)\n",
        "    # Learn every UPDATE_EVERY time steps.\n",
        "    self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "    if self.t_step == 0:\n",
        "       # If enough samples are available in memory, get random subset and learn\n",
        "      if len(self.memory) > BATCH_SIZE:\n",
        "        experiences = self.memory.sample()\n",
        "        self.learn(experiences, GAMMA)\n",
        "\n",
        "  def act(self, state, eps = 0):\n",
        "    \"\"\"Returns actions for given state as per current policy.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state (array_like): current state\n",
        "            eps (float): epsilon, for epsilon-greedy action selection\n",
        "        \"\"\"\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "    self.qnetwork_local.eval()\n",
        "    with torch.no_grad():\n",
        "      action_values = self.qnetwork_local(state)\n",
        "    self.qnetwork_local.train()\n",
        "\n",
        "    if random.random() > eps:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "\n",
        "  def learn(self, experiences, gamma):\n",
        "    states, actions, reward, next_states, dones = experiences\n",
        "    # Get max predicted Q values (for next states) from target model\n",
        "    Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    # Compute Q targets for current states\n",
        "    Q_targets = reward + (gamma * Q_targets_next * (1 - dones))\n",
        "    # Get expected Q values from local model\n",
        "    Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "    #Compute loss \n",
        "    loss = F.mse_loss(Q_expected, Q_targets)\n",
        "    #Minimize loss\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    # ------------------- update target network ------------------- #\n",
        "    self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)     \n",
        "  \n",
        "  def soft_update(self, local_model, target_model, tau):\n",
        "    \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            local_model (PyTorch model): weights will be copied from\n",
        "            target_model (PyTorch model): weights will be copied to\n",
        "            tau (float): interpolation parameter \n",
        "        \"\"\"\n",
        "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "      target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l2-DiGu1Mcb"
      },
      "source": [
        "agent = Agent(state_size=4, action_size=2, seed=0)\n",
        "\n",
        "state = env.reset()\n",
        "for j in range(200):\n",
        "    action = agent.act(state)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    if done:\n",
        "        break \n",
        "        \n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgn83DIU1uuV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "1cdf59c2-bea8-4701-c9bd-9bbce25e0c8d"
      },
      "source": [
        "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "  \"\"\"Deep Q-Learning.\n",
        "  \n",
        "  Params\n",
        "  ======\n",
        "      n_episodes (int): maximum number of training episodes\n",
        "      max_t (int): maximum number of timesteps per episode\n",
        "      eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
        "      eps_end (float): minimum value of epsilon\n",
        "      eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
        "  \"\"\"\n",
        "  scores = []\n",
        "  scores_window = deque(maxlen = 100)\n",
        "  eps = eps_start\n",
        "\n",
        "  for i_episode in range(1, n_episodes+1):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    for t in range(max_t):\n",
        "      action = agent.act(state, eps)\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      agent.step(state, action, reward, next_state, done)\n",
        "      state = next_state\n",
        "      score += reward\n",
        "      if done :\n",
        "        break\n",
        "\n",
        "    scores_window.append(score)\n",
        "    scores.append(score)\n",
        "    eps = max(eps_end, eps_decay * eps)\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "    if i_episode % 100 == 0:\n",
        "      print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "    if np.mean(scores_window)>=200.0:\n",
        "      print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
        "      torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
        "      break\n",
        "  return scores\n",
        "\n",
        "\n",
        "scores = dqn()\n",
        "\n",
        "\n",
        "\n",
        "# plot the scores\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(len(scores)), scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 100\tAverage Score: 31.18\n",
            "Episode 200\tAverage Score: 62.66\n",
            "Episode 300\tAverage Score: 147.34\n",
            "Episode 397\tAverage Score: 200.08\n",
            "Environment solved in 297 episodes!\tAverage Score: 200.08\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZhUPCmZmVvGgYGZmFQ8KZmZWuQW+iDtJjlW3kgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7MEw8q73Hbc"
      },
      "source": [
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Segment tree for Prioritized Replay Buffer.\"\"\"\n",
        "\n",
        "import operator\n",
        "from typing import Callable\n",
        "\n",
        "\n",
        "class SegmentTree:\n",
        "    \"\"\" Create SegmentTree.\n",
        "    Taken from OpenAI baselines github repository:\n",
        "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
        "    Attributes:\n",
        "        capacity (int)\n",
        "        tree (list)\n",
        "        operation (function)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, capacity: int, operation: Callable, init_value: float):\n",
        "        \"\"\"Initialization.\n",
        "        Args:\n",
        "            capacity (int)\n",
        "            operation (function)\n",
        "            init_value (float)\n",
        "        \"\"\"\n",
        "        assert (\n",
        "            capacity > 0 and capacity & (capacity - 1) == 0\n",
        "        ), \"capacity must be positive and a power of 2.\"\n",
        "        self.capacity = capacity\n",
        "        self.tree = [init_value for _ in range(2 * capacity)]\n",
        "        self.operation = operation\n",
        "\n",
        "    def _operate_helper(\n",
        "        self, start: int, end: int, node: int, node_start: int, node_end: int\n",
        "    ) -> float:\n",
        "        \"\"\"Returns result of operation in segment.\"\"\"\n",
        "        if start == node_start and end == node_end:\n",
        "            return self.tree[node]\n",
        "        mid = (node_start + node_end) // 2\n",
        "        if end <= mid:\n",
        "            return self._operate_helper(start, end, 2 * node, node_start, mid)\n",
        "        else:\n",
        "            if mid + 1 <= start:\n",
        "                return self._operate_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
        "            else:\n",
        "                return self.operation(\n",
        "                    self._operate_helper(start, mid, 2 * node, node_start, mid),\n",
        "                    self._operate_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end),\n",
        "                )\n",
        "\n",
        "    def operate(self, start: int = 0, end: int = 0) -> float:\n",
        "        \"\"\"Returns result of applying `self.operation`.\"\"\"\n",
        "        if end <= 0:\n",
        "            end += self.capacity\n",
        "        end -= 1\n",
        "\n",
        "        return self._operate_helper(start, end, 1, 0, self.capacity - 1)\n",
        "\n",
        "    def __setitem__(self, idx: int, val: float):\n",
        "        \"\"\"Set value in tree.\"\"\"\n",
        "        idx += self.capacity\n",
        "        self.tree[idx] = val\n",
        "\n",
        "        idx //= 2\n",
        "        while idx >= 1:\n",
        "            self.tree[idx] = self.operation(self.tree[2 * idx], self.tree[2 * idx + 1])\n",
        "            idx //= 2\n",
        "\n",
        "    def __getitem__(self, idx: int) -> float:\n",
        "        \"\"\"Get real value in leaf node of tree.\"\"\"\n",
        "        assert 0 <= idx < self.capacity\n",
        "\n",
        "        return self.tree[self.capacity + idx]\n",
        "\n",
        "\n",
        "class SumSegmentTree(SegmentTree):\n",
        "    \"\"\" Create SumSegmentTree.\n",
        "    Taken from OpenAI baselines github repository:\n",
        "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, capacity: int):\n",
        "        \"\"\"Initialization.\n",
        "        Args:\n",
        "            capacity (int)\n",
        "        \"\"\"\n",
        "        super(SumSegmentTree, self).__init__(\n",
        "            capacity=capacity, operation=operator.add, init_value=0.0\n",
        "        )\n",
        "\n",
        "    def sum(self, start: int = 0, end: int = 0) -> float:\n",
        "        \"\"\"Returns arr[start] + ... + arr[end].\"\"\"\n",
        "        return super(SumSegmentTree, self).operate(start, end)\n",
        "\n",
        "    def retrieve(self, upperbound: float) -> int:\n",
        "        \"\"\"Find the highest index `i` about upper bound in the tree\"\"\"\n",
        "        # TODO: Check assert case and fix bug\n",
        "        assert 0 <= upperbound <= self.sum() + 1e-5, \"upperbound: {}\".format(upperbound)\n",
        "\n",
        "        idx = 1\n",
        "\n",
        "        while idx < self.capacity:  # while non-leaf\n",
        "            left = 2 * idx\n",
        "            right = left + 1\n",
        "            if self.tree[left] > upperbound:\n",
        "                idx = 2 * idx\n",
        "            else:\n",
        "                upperbound -= self.tree[left]\n",
        "                idx = right\n",
        "        return idx - self.capacity\n",
        "\n",
        "\n",
        "class MinSegmentTree(SegmentTree):\n",
        "    \"\"\" Create SegmentTree.\n",
        "    Taken from OpenAI baselines github repository:\n",
        "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, capacity: int):\n",
        "        \"\"\"Initialization.\n",
        "        Args:\n",
        "            capacity (int)\n",
        "        \"\"\"\n",
        "        super(MinSegmentTree, self).__init__(\n",
        "            capacity=capacity, operation=min, init_value=float(\"inf\")\n",
        "        )\n",
        "\n",
        "    def min(self, start: int = 0, end: int = 0) -> float:\n",
        "        \"\"\"Returns min(arr[start], ...,  arr[end]).\"\"\"\n",
        "        return super(MinSegmentTree, self).operate(start, end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfAIlRC_FVqR"
      },
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "    \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "    \"\"\"\n",
        "    self.action_size = action_size\n",
        "    self.memory = deque(maxlen=buffer_size) \n",
        "    self.max_size = buffer_size \n",
        "    self.batch_size = batch_size\n",
        "    self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "    self.seed = random.seed(seed)\n",
        "\n",
        "\n",
        "  def add(self, state, action, reward, next_state, done):\n",
        "    \"\"\"Add a new experience to memory.\"\"\"\n",
        "\n",
        "    e = self.experience(state, action, reward, next_state, done)\n",
        "    self.memory.append(e)\n",
        "\n",
        "  def sample(self):\n",
        "\n",
        "    experiences = random.sample(self.memory, k=self.batch_size)\n",
        "    states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "    actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "    rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "    next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "    dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "    return (states, actions, reward, next_states, dones)\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"Return the current size of internal memory.\"\"\"\n",
        "    return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE731tiK1f53"
      },
      "source": [
        "import random\n",
        "from collections import namedtuple, deque\n",
        "import numpy as np\n",
        "#from numpy.random import choice\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class PrioritizedReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, experiences_per_sampling, seed, compute_weights):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            experiences_per_sampling (int): number of experiences to sample during a sampling iteration\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.experiences_per_sampling = experiences_per_sampling\n",
        "\n",
        "        self.alpha = 0.5\n",
        "        self.alpha_decay_rate = 0.99\n",
        "        self.beta = 0.5\n",
        "        self.beta_growth_rate = 1.001\n",
        "        self.seed = random.seed(seed)\n",
        "        self.compute_weights = compute_weights\n",
        "        self.experience_count = 0\n",
        "\n",
        "        self.experience = namedtuple(\"Experience\",\n",
        "                                     field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.data = namedtuple(\"Data\",\n",
        "                               field_names=[\"priority\", \"probability\", \"weight\", \"index\"])\n",
        "\n",
        "        indexes = []\n",
        "        datas = []\n",
        "        for i in range(buffer_size):\n",
        "            indexes.append(i)\n",
        "            d = self.data(0, 0, 0, i)\n",
        "            datas.append(d)\n",
        "\n",
        "        self.memory = {key: self.experience for key in indexes}\n",
        "        self.memory_data = {key: data for key, data in zip(indexes, datas)}\n",
        "        self.sampled_batches = []\n",
        "        self.current_batch = 0\n",
        "        self.priorities_sum_alpha = 0\n",
        "        self.priorities_max = 1\n",
        "        self.weights_max = 1\n",
        "\n",
        "    def update_priorities(self, tds, indices):\n",
        "        for td, index in zip(tds, indices):\n",
        "            N = min(self.experience_count, self.buffer_size)\n",
        "\n",
        "            updated_priority = td[0]\n",
        "            if updated_priority > self.priorities_max:\n",
        "                self.priorities_max = updated_priority\n",
        "\n",
        "            if self.compute_weights:\n",
        "                updated_weight = ((N * updated_priority) ** (-self.beta)) / self.weights_max\n",
        "                if updated_weight > self.weights_max:\n",
        "                    self.weights_max = updated_weight\n",
        "            else:\n",
        "                updated_weight = 1\n",
        "\n",
        "            old_priority = self.memory_data[index].priority\n",
        "            self.priorities_sum_alpha += updated_priority ** self.alpha - old_priority ** self.alpha\n",
        "            updated_probability = td[0] ** self.alpha / self.priorities_sum_alpha\n",
        "            data = self.data(updated_priority, updated_probability, updated_weight, index)\n",
        "            self.memory_data[index] = data\n",
        "\n",
        "    def update_memory_sampling(self):\n",
        "        \"\"\"Randomly sample X batches of experiences from memory.\"\"\"\n",
        "        # X is the number of steps before updating memory\n",
        "        self.current_batch = 0\n",
        "        values = list(self.memory_data.values())\n",
        "        print([data.probability for data in values][0],len(self.memory_data),\"!!!!!!!\")\n",
        "        random_values = np.random.choice(self.memory_data, self.experiences_per_sampling,\n",
        "                                       p=[data.probability for data in values])\n",
        "        self.sampled_batches = [random_values[i:i + self.batch_size]\n",
        "                                for i in range(0, len(random_values), self.batch_size)]\n",
        "\n",
        "    def update_parameters(self):\n",
        "        self.alpha *= self.alpha_decay_rate\n",
        "        self.beta *= self.beta_growth_rate\n",
        "        if self.beta > 1:\n",
        "            self.beta = 1\n",
        "        N = min(self.experience_count, self.buffer_size)\n",
        "        self.priorities_sum_alpha = 0\n",
        "        sum_prob_before = 0\n",
        "        for element in self.memory_data.values():\n",
        "            sum_prob_before += element.probability\n",
        "            self.priorities_sum_alpha += element.priority ** self.alpha\n",
        "        sum_prob_after = 0\n",
        "        for element in self.memory_data.values():\n",
        "            probability = element.priority ** self.alpha / self.priorities_sum_alpha\n",
        "            sum_prob_after += probability\n",
        "            weight = 1\n",
        "            if self.compute_weights:\n",
        "                weight = ((N * element.probability) ** (-self.beta)) / self.weights_max\n",
        "            d = self.data(element.priority, probability, weight, element.index)\n",
        "            self.memory_data[element.index] = d\n",
        "        print(\"sum_prob before\", sum_prob_before)\n",
        "        print(\"sum_prob after : \", sum_prob_after)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        self.experience_count += 1\n",
        "        index = self.experience_count % self.buffer_size\n",
        "\n",
        "        if self.experience_count > self.buffer_size:\n",
        "            temp = self.memory_data[index]\n",
        "            self.priorities_sum_alpha -= temp.priority ** self.alpha\n",
        "            if temp.priority == self.priorities_max:\n",
        "                self.memory_data[index].priority = 0\n",
        "                self.priorities_max = max(self.memory_data.items(), key=operator.itemgetter(1)).priority\n",
        "            if self.compute_weights:\n",
        "                if temp.weight == self.weights_max:\n",
        "                    self.memory_data[index].weight = 0\n",
        "                    self.weights_max = max(self.memory_data.items(), key=operator.itemgetter(2)).weight\n",
        "\n",
        "        priority = self.priorities_max\n",
        "        weight = self.weights_max\n",
        "        self.priorities_sum_alpha += priority ** self.alpha\n",
        "        probability = priority ** self.alpha / self.priorities_sum_alpha\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory[index] = e\n",
        "        d = self.data(priority, probability, weight, index)\n",
        "        self.memory_data[index] = d\n",
        "\n",
        "    def sample(self):\n",
        "        print(len(self.sampled_batches),self.current_batch )\n",
        "        sampled_batch = self.sampled_batches[self.current_batch]\n",
        "        self.current_batch += 1\n",
        "        experiences = []\n",
        "        weights = []\n",
        "        indices = []\n",
        "\n",
        "        for data in sampled_batch:\n",
        "            experiences.append(self.memory.get(data.index))\n",
        "            weights.append(data.weight)\n",
        "            indices.append(data.index)\n",
        "\n",
        "        states = torch.from_numpy(\n",
        "            np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(\n",
        "            np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(\n",
        "            np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(\n",
        "            np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(\n",
        "            np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones, weights, indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz2pTPbONZsm"
      },
      "source": [
        "class QNetwork(nn.Module):\n",
        "  \"\"\"Actor (Policy) Model.\"\"\"\n",
        "  def __init__(self, state_size, action_size, seed, fc1_unit=64, fc2_unit=64):\n",
        "    \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "    \"\"\"\n",
        "    super(QNetwork, self).__init__()\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    self.fc1 = nn.Linear(state_size, fc1_unit)\n",
        "    self.fc2 = nn.Linear(fc1_unit, fc2_unit)\n",
        "    self.fc3 = nn.Linear(fc2_unit, action_size)\n",
        "\n",
        "  def forward(self, state):\n",
        "    \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "    x = F.relu(self.fc1(state))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    return self.fc3(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdLKJnX0Ovfj"
      },
      "source": [
        "UPDATE_NN_EVERY = 1 \n",
        "\n",
        "# prioritized experience replay\n",
        "UPDATE_MEM_EVERY = 20  # how often to update the priorities\n",
        "UPDATE_MEM_PAR_EVERY = 1000  # how often to update the hyperparameters\n",
        "EXPERIENCES_PER_SAMPLING = int(math.ceil(BATCH_SIZE * UPDATE_MEM_EVERY / UPDATE_NN_EVERY))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv8g2WLTPkNW"
      },
      "source": [
        "class Agent():\n",
        "  \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "  def __init__(self, state_size, action_size, seed):\n",
        "    \"\"\"Initialize an Agent object.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            seed (int): random seed\n",
        "    \"\"\"\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.seed = random.seed(seed)\n",
        "\n",
        "    self.memory = PrioritizedReplayBuffer(\n",
        "            self.action_size, BUFFER_SIZE, BATCH_SIZE, BATCH_SIZE, seed, False)\n",
        "    # Initialize time step (for updating every UPDATE_NN_EVERY steps)\n",
        "    self.t_step_nn = 0\n",
        "    # Initialize time step (for updating every UPDATE_MEM_PAR_EVERY steps)\n",
        "    self.t_step_mem_par = 0\n",
        "    # Initialize time step (for updating every UPDATE_MEM_EVERY steps)\n",
        "    self.t_step_mem = 0\n",
        "\n",
        "    #Q-Networks\n",
        "    self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
        "    self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
        "    self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "    self.t_step = 0\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    # Save experience in replay memory\n",
        "    self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "    # Learn every UPDATE_NN_EVERY time steps.\n",
        "    self.t_step_nn = (self.t_step_nn + 1) % UPDATE_NN_EVERY\n",
        "    self.t_step_mem = (self.t_step_mem + 1) % UPDATE_MEM_EVERY\n",
        "    self.t_step_mem_par = (self.t_step_mem_par + 1) % UPDATE_MEM_PAR_EVERY\n",
        "\n",
        "    if self.t_step_nn == 0:\n",
        "      # If enough samples are available in memory, get random subset and learn\n",
        "      if self.memory.experience_count > EXPERIENCES_PER_SAMPLING:\n",
        "        sampling = self.memory.sample()\n",
        "        self.learn(sampling, GAMMA)\n",
        "\n",
        "  def act(self, state, eps = 0):\n",
        "    \"\"\"Returns actions for given state as per current policy.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state (array_like): current state\n",
        "            eps (float): epsilon, for epsilon-greedy action selection\n",
        "        \"\"\"\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "    self.qnetwork_local.eval()\n",
        "    with torch.no_grad():\n",
        "      action_values = self.qnetwork_local(state)\n",
        "    self.qnetwork_local.train()\n",
        "\n",
        "    if random.random() > eps:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "\n",
        "  def learn(self, experiences, gamma):\n",
        "    states, actions, reward, next_states, dones = experiences\n",
        "    # Get max predicted Q values (for next states) from target model\n",
        "    Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    # Compute Q targets for current states\n",
        "    Q_targets = reward + (gamma * Q_targets_next * (1 - dones))\n",
        "    # Get expected Q values from local model\n",
        "    Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "    #Compute loss \n",
        "    loss = F.mse_loss(Q_expected, Q_targets)\n",
        "    #Minimize loss\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    # ------------------- update target network ------------------- #\n",
        "    self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)     \n",
        "  \n",
        "  def soft_update(self, local_model, target_model, tau):\n",
        "    \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            local_model (PyTorch model): weights will be copied from\n",
        "            target_model (PyTorch model): weights will be copied to\n",
        "            tau (float): interpolation parameter \n",
        "        \"\"\"\n",
        "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "      target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch2KlJSqRz9h"
      },
      "source": [
        "agent = Agent(state_size=4, action_size=2, seed=0)\n",
        "\n",
        "state = env.reset()\n",
        "for j in range(200):\n",
        "    action = agent.act(state)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    if done:\n",
        "        break \n",
        "        \n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "Wk4XRxhTRmew",
        "outputId": "82cd6ca8-93ea-499a-9571-f4abcad3e181"
      },
      "source": [
        "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "  \"\"\"Deep Q-Learning.\n",
        "  \n",
        "  Params\n",
        "  ======\n",
        "      n_episodes (int): maximum number of training episodes\n",
        "      max_t (int): maximum number of timesteps per episode\n",
        "      eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
        "      eps_end (float): minimum value of epsilon\n",
        "      eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
        "  \"\"\"\n",
        "  scores = []\n",
        "  scores_window = deque(maxlen = 100)\n",
        "  eps = eps_start\n",
        "\n",
        "  for i_episode in range(1, n_episodes+1):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    for t in range(max_t):\n",
        "      action = agent.act(state, eps)\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      agent.step(state, action, reward, next_state, done)\n",
        "      state = next_state\n",
        "      score += reward\n",
        "      if done :\n",
        "        break\n",
        "\n",
        "    scores_window.append(score)\n",
        "    scores.append(score)\n",
        "    eps = max(eps_end, eps_decay * eps)\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "    if i_episode % 100 == 0:\n",
        "      print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "    if np.mean(scores_window)>=200.0:\n",
        "      print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
        "      torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
        "      break\n",
        "  return scores\n",
        "\n",
        "\n",
        "scores = dqn()\n",
        "\n",
        "\n",
        "\n",
        "# plot the scores\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(len(scores)), scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 57\tAverage Score: 22.350 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-9e3e6aabd0f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-9e3e6aabd0f8>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m       \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-2cb2a24e3fd6>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0;31m# If enough samples are available in memory, get random subset and learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mEXPERIENCES_PER_SAMPLING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0msampling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-d6b5456f5358>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampled_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_batch\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0msampled_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampled_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFXbg6OzRtDC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}